{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from nmt.datasets import Vocab, batch_iter\n",
    "from nmt.networks import CharEmbedding, Encoder\n",
    "from nmt.layers import Attention\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup something to work with\n",
    "\n",
    "sentences_words_src = [\n",
    "    ['Human:', 'What', 'do', 'we', 'want?'],\n",
    "    ['Computer:', 'Natural', 'language', 'processing!'],\n",
    "    ['Human:', 'When', 'do', 'we', 'want', 'it?'],\n",
    "    ['Computer:', 'When', 'do', 'we', 'want', 'what?']\n",
    "]\n",
    "\n",
    "sentences_words_tgt = [\n",
    "    ['<s>', 'Human:', 'What', 'do', 'we', 'want?', '</s>'],\n",
    "    ['<s>', 'Computer:', 'Natural', 'language', 'processing!', '</s>'],\n",
    "    ['<s>', 'Human:', 'When', 'do', 'we', 'want', 'it?', '</s>'],\n",
    "    ['<s>', 'Computer:', 'When', 'do', 'we', 'want', 'what?', '</s>']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing source vocab\nVocab Store: Tokens [size=17],                 Characters [size=97]\nInitializing target vocab\nVocab Store: Tokens [size=17],                 Characters [size=97]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab.build(sentences_words_src, sentences_words_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['Computer:', 'When', 'do', 'we', 'want', 'what?'], ['Human:', 'When', 'do', 'we', 'want', 'it?'], ['Human:', 'What', 'do', 'we', 'want?'], ['Computer:', 'Natural', 'language', 'processing!']]\n"
     ]
    }
   ],
   "source": [
    "data = list(zip(sentences_words_src, sentences_words_tgt))\n",
    "data_generator = batch_iter(\n",
    "    data=data,\n",
    "    batch_size=4,\n",
    "    shuffle=True\n",
    ")\n",
    "batch_src, batch_tgt = next(data_generator)\n",
    "print(batch_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[6, 6, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "source_length = [len(sent) for sent in batch_src]\n",
    "print(source_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "src char tensor size = torch.Size([6, 4, 21]); tgt char tensor size = torch.Size([8, 4, 21])\n"
     ]
    }
   ],
   "source": [
    "char_tensors_src = vocab.src.to_tensor(batch_src, tokens=False)\n",
    "char_tensors_tgt = vocab.tgt.to_tensor(batch_tgt, tokens=False)\n",
    "print(f\"src char tensor size = {char_tensors_src.size()}; tgt char tensor size = {char_tensors_tgt.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    num_embeddings=vocab.src.length(tokens=False),\n",
    "    embedding_dim=300,\n",
    "    char_padding_idx=vocab.src.pad_char_idx,\n",
    "    hidden_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([4, 6, 2048]), torch.Size([4, 1024]), torch.Size([4, 1024]))"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "char_enc_hidden, (char_hidden, char_cell) = encoder(char_tensors_src, source_length)\n",
    "char_enc_hidden.shape, char_hidden.shape, char_cell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = Attention(in_features=2048, out_features=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4, 1, 6]) torch.Size([4, 2048])\n"
     ]
    }
   ],
   "source": [
    "alpha_t, a_t = attention(char_enc_hidden, char_hidden)\n",
    "print(alpha_t.shape, a_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_embeddings: int,\n",
    "                 embedding_dim: int, hidden_size: int,\n",
    "                 char_padding_idx: int, char_embedding_dim: int = 50, \n",
    "                 bias: bool = True, dropout_prob: float = 0.3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = CharEmbedding(\n",
    "            num_embeddings=num_embeddings,\n",
    "            char_embedding_dim=char_embedding_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            char_padding_idx=char_padding_idx\n",
    "        )\n",
    "        self.attention = Attention(\n",
    "            in_features=hidden_size*2,\n",
    "            out_features=hidden_size\n",
    "        )\n",
    "        self.decoder = nn.LSTMCell(\n",
    "            input_size=embedding_dim + hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            bias=bias\n",
    "        )\n",
    "        self.combined_projection = nn.Linear(\n",
    "            in_features=hidden_size*3,\n",
    "            out_features=hidden_size,\n",
    "            bias=False\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor,\n",
    "                enc_hidden: torch.Tensor,\n",
    "                dec_init_state: Tuple[torch.Tensor, torch.Tensor],\n",
    "                o_prev: torch.Tensor,\n",
    "                enc_masks: torch.Tensor = None) -> torch.Tensor:\n",
    "\n",
    "        dec_state = dec_init_state\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        Ybar_t = torch.cat([x.squeeze(dim=0), o_prev], dim=1)\n",
    "\n",
    "        dec_state = self.decoder(Ybar_t, dec_state)\n",
    "        dec_hidden, dec_cell = dec_state\n",
    "\n",
    "        attention_scores, context_vector = self.attention(\n",
    "            enc_hidden, dec_hidden, enc_masks)\n",
    "\n",
    "        U_t = torch.cat([context_vector, dec_hidden], dim=1)\n",
    "        V_t = self.combined_projection(U_t)\n",
    "        output = self.dropout(V_t.tanh())\n",
    "\n",
    "        return output, dec_state, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(\n",
    "    num_embeddings=vocab.tgt.length(tokens=False),\n",
    "    embedding_dim=300,\n",
    "    char_padding_idx=vocab.tgt.pad_char_idx,\n",
    "    hidden_size=1024\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tensors_tgt = char_tensors_tgt[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, sent_length, _ = char_enc_hidden.size()\n",
    "o_prev = torch.zeros(batch_size, 1024, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_outputs = []\n",
    "for y_t in torch.split(char_tensors_tgt, 1, dim=0):\n",
    "    o_prev, dec_state, _ = decoder(y_t, char_enc_hidden, (char_hidden, char_cell), o_prev)\n",
    "    combined_outputs.append(o_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_outputs = torch.stack(combined_outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = nn.Linear(\n",
    "            in_features=1024,\n",
    "            out_features=len(vocab.tgt),\n",
    "            bias=False\n",
    "        )\n",
    "P = F.log_softmax(target_layer(combined_outputs), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 17])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}