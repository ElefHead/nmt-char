{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from nmt.models import NMT\n",
    "from nmt.datasets import read_corpus, batch_iter, Vocab\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = Path(\"..\") / \"nmt\" / \"datasets\" / \"data\"\n",
    "en_es_data_loc = data_loc / \"en_es_data\"\n",
    "train_data_src_path = en_es_data_loc / \"train_tiny.es\"\n",
    "train_data_tgt_path = en_es_data_loc / \"train_tiny.en\"\n",
    "dev_data_src_path = en_es_data_loc / \"dev_tiny.es\"\n",
    "dev_data_tgt_path = en_es_data_loc / \"dev_tiny.en\"\n",
    "vocab_path = data_loc / \"vocab_tiny_q2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = read_corpus(train_data_src_path)\n",
    "train_tgt = read_corpus(train_data_tgt_path, is_target=True)\n",
    "vocab = Vocab.load(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(26, 32)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "len(vocab.src), len(vocab.tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_src = read_corpus(dev_data_src_path)\n",
    "valid_tgt = read_corpus(dev_data_tgt_path, is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=2\n",
    "MAX_EPOCH=201\n",
    "SEED=42\n",
    "EMBEDDING_SIZE=256\n",
    "HIDDEN_SIZE=256\n",
    "GRAD_CLIP=5.0\n",
    "UNIFORM_INIT=0.1\n",
    "USE_CHAR_DECODER=True\n",
    "LEARNING_RATE=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMT(\n",
    "    vocab=vocab,\n",
    "    embedding_dim=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    use_char_decoder=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NMT(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): CharEmbedding(\n",
       "      (char_embed): Embedding(97, 50, padding_idx=0)\n",
       "      (cnn_embed): CharCNNEmbedding(\n",
       "        (conv): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
       "        (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
       "      )\n",
       "      (highway): Highway(\n",
       "        (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (gate): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (encoder): LSTM(256, 256, num_layers=2, bidirectional=True)\n",
       "    (hidden_projection): Linear(in_features=512, out_features=256, bias=False)\n",
       "    (cell_projection): Linear(in_features=512, out_features=256, bias=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): CharEmbedding(\n",
       "      (char_embed): Embedding(97, 50, padding_idx=0)\n",
       "      (cnn_embed): CharCNNEmbedding(\n",
       "        (conv): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
       "        (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
       "      )\n",
       "      (highway): Highway(\n",
       "        (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (gate): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (attention): Attention(\n",
       "      (linear): Linear(in_features=512, out_features=256, bias=False)\n",
       "    )\n",
       "    (decoder): LSTMCell(512, 256)\n",
       "    (combined_projection): Linear(in_features=768, out_features=256, bias=False)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (target_layer): Linear(in_features=256, out_features=32, bias=False)\n",
       "  (char_decoder): CharDecoder(\n",
       "    (embedding): Embedding(97, 50, padding_idx=0)\n",
       "    (char_decoder): LSTM(50, 256)\n",
       "    (linear): Linear(in_features=256, out_features=97, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "uniformly initialize parameters [-0.100000, +0.100000]\n"
     ]
    }
   ],
   "source": [
    "uniform_init = UNIFORM_INIT\n",
    "if np.abs(uniform_init) > 0.:\n",
    "    print('uniformly initialize parameters [-%f, +%f]' %\n",
    "            (uniform_init, uniform_init), file=sys.stderr)\n",
    "    for p in model.parameters():\n",
    "        p.data.uniform_(-uniform_init, uniform_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 000 - Cumulative loss: 76.07486724853516\n",
      "Epoch: 001 - Cumulative loss: 72.79556274414062\n",
      "Epoch: 002 - Cumulative loss: 69.85139465332031\n",
      "Epoch: 003 - Cumulative loss: 68.46512603759766\n",
      "Epoch: 004 - Cumulative loss: 66.44754791259766\n",
      "Epoch: 005 - Cumulative loss: 63.35455322265625\n",
      "Epoch: 006 - Cumulative loss: 60.66731643676758\n",
      "Epoch: 007 - Cumulative loss: 57.88811111450195\n",
      "Epoch: 008 - Cumulative loss: 54.35834503173828\n",
      "Epoch: 009 - Cumulative loss: 51.49571990966797\n",
      "Epoch: 010 - Cumulative loss: 49.374305725097656\n",
      "Epoch: 011 - Cumulative loss: 47.99576187133789\n",
      "Epoch: 012 - Cumulative loss: 47.189720153808594\n",
      "Epoch: 013 - Cumulative loss: 46.40201950073242\n",
      "Epoch: 014 - Cumulative loss: 46.165184020996094\n",
      "Epoch: 015 - Cumulative loss: 45.659889221191406\n",
      "Epoch: 016 - Cumulative loss: 45.351680755615234\n",
      "Epoch: 017 - Cumulative loss: 44.927940368652344\n",
      "Epoch: 018 - Cumulative loss: 44.625667572021484\n",
      "Epoch: 019 - Cumulative loss: 44.373634338378906\n",
      "Epoch: 020 - Cumulative loss: 44.22616958618164\n",
      "Epoch: 021 - Cumulative loss: 44.32121658325195\n",
      "Epoch: 022 - Cumulative loss: 43.90202331542969\n",
      "Epoch: 023 - Cumulative loss: 43.74616241455078\n",
      "Epoch: 024 - Cumulative loss: 43.59789276123047\n",
      "Epoch: 025 - Cumulative loss: 43.58741760253906\n",
      "Epoch: 026 - Cumulative loss: 43.43640899658203\n",
      "Epoch: 027 - Cumulative loss: 43.004852294921875\n",
      "Epoch: 028 - Cumulative loss: 43.049102783203125\n",
      "Epoch: 029 - Cumulative loss: 42.79302978515625\n",
      "Epoch: 030 - Cumulative loss: 42.723697662353516\n",
      "Epoch: 031 - Cumulative loss: 42.640830993652344\n",
      "Epoch: 032 - Cumulative loss: 42.59900665283203\n",
      "Epoch: 033 - Cumulative loss: 42.537208557128906\n",
      "Epoch: 034 - Cumulative loss: 42.447166442871094\n",
      "Epoch: 035 - Cumulative loss: 42.02731704711914\n",
      "Epoch: 036 - Cumulative loss: 42.088321685791016\n",
      "Epoch: 037 - Cumulative loss: 41.73051452636719\n",
      "Epoch: 038 - Cumulative loss: 42.02776336669922\n",
      "Epoch: 039 - Cumulative loss: 41.4892692565918\n",
      "Epoch: 040 - Cumulative loss: 41.545772552490234\n",
      "Epoch: 041 - Cumulative loss: 41.089439392089844\n",
      "Epoch: 042 - Cumulative loss: 41.33095169067383\n",
      "Epoch: 043 - Cumulative loss: 41.10841751098633\n",
      "Epoch: 044 - Cumulative loss: 41.11506652832031\n",
      "Epoch: 045 - Cumulative loss: 40.896148681640625\n",
      "Epoch: 046 - Cumulative loss: 40.70880889892578\n",
      "Epoch: 047 - Cumulative loss: 40.488826751708984\n",
      "Epoch: 048 - Cumulative loss: 40.317283630371094\n",
      "Epoch: 049 - Cumulative loss: 40.43848419189453\n",
      "Epoch: 050 - Cumulative loss: 40.12793731689453\n",
      "Epoch: 051 - Cumulative loss: 39.757408142089844\n",
      "Epoch: 052 - Cumulative loss: 39.60636520385742\n",
      "Epoch: 053 - Cumulative loss: 39.4282341003418\n",
      "Epoch: 054 - Cumulative loss: 39.292808532714844\n",
      "Epoch: 055 - Cumulative loss: 39.15226745605469\n",
      "Epoch: 056 - Cumulative loss: 39.05596923828125\n",
      "Epoch: 057 - Cumulative loss: 38.24205780029297\n",
      "Epoch: 058 - Cumulative loss: 38.01918029785156\n",
      "Epoch: 059 - Cumulative loss: 37.827857971191406\n",
      "Epoch: 060 - Cumulative loss: 37.45052719116211\n",
      "Epoch: 061 - Cumulative loss: 37.48128128051758\n",
      "Epoch: 062 - Cumulative loss: 37.225311279296875\n",
      "Epoch: 063 - Cumulative loss: 37.296077728271484\n",
      "Epoch: 064 - Cumulative loss: 36.48133087158203\n",
      "Epoch: 065 - Cumulative loss: 36.05299377441406\n",
      "Epoch: 066 - Cumulative loss: 35.905738830566406\n",
      "Epoch: 067 - Cumulative loss: 35.652137756347656\n",
      "Epoch: 068 - Cumulative loss: 35.69379425048828\n",
      "Epoch: 069 - Cumulative loss: 35.329429626464844\n",
      "Epoch: 070 - Cumulative loss: 34.61544418334961\n",
      "Epoch: 071 - Cumulative loss: 34.71602249145508\n",
      "Epoch: 072 - Cumulative loss: 34.108272552490234\n",
      "Epoch: 073 - Cumulative loss: 33.851722717285156\n",
      "Epoch: 074 - Cumulative loss: 33.39196014404297\n",
      "Epoch: 075 - Cumulative loss: 33.11312484741211\n",
      "Epoch: 076 - Cumulative loss: 32.81014633178711\n",
      "Epoch: 077 - Cumulative loss: 32.26409912109375\n",
      "Epoch: 078 - Cumulative loss: 32.07350158691406\n",
      "Epoch: 079 - Cumulative loss: 31.772573471069336\n",
      "Epoch: 080 - Cumulative loss: 31.465951919555664\n",
      "Epoch: 081 - Cumulative loss: 31.131887435913086\n",
      "Epoch: 082 - Cumulative loss: 30.5319881439209\n",
      "Epoch: 083 - Cumulative loss: 30.45844078063965\n",
      "Epoch: 084 - Cumulative loss: 30.189334869384766\n",
      "Epoch: 085 - Cumulative loss: 30.040569305419922\n",
      "Epoch: 086 - Cumulative loss: 29.55831527709961\n",
      "Epoch: 087 - Cumulative loss: 29.32390785217285\n",
      "Epoch: 088 - Cumulative loss: 28.651086807250977\n",
      "Epoch: 089 - Cumulative loss: 28.938629150390625\n",
      "Epoch: 090 - Cumulative loss: 28.527069091796875\n",
      "Epoch: 091 - Cumulative loss: 28.114765167236328\n",
      "Epoch: 092 - Cumulative loss: 27.49044418334961\n",
      "Epoch: 093 - Cumulative loss: 27.512060165405273\n",
      "Epoch: 094 - Cumulative loss: 26.88238525390625\n",
      "Epoch: 095 - Cumulative loss: 26.789199829101562\n",
      "Epoch: 096 - Cumulative loss: 26.57294273376465\n",
      "Epoch: 097 - Cumulative loss: 26.413955688476562\n",
      "Epoch: 098 - Cumulative loss: 25.900583267211914\n",
      "Epoch: 099 - Cumulative loss: 25.825109481811523\n",
      "Epoch: 100 - Cumulative loss: 25.105587005615234\n",
      "Epoch: 101 - Cumulative loss: 25.034631729125977\n",
      "Epoch: 102 - Cumulative loss: 25.494081497192383\n",
      "Epoch: 103 - Cumulative loss: 24.759042739868164\n",
      "Epoch: 104 - Cumulative loss: 24.415721893310547\n",
      "Epoch: 105 - Cumulative loss: 24.340862274169922\n",
      "Epoch: 106 - Cumulative loss: 23.619396209716797\n",
      "Epoch: 107 - Cumulative loss: 23.472705841064453\n",
      "Epoch: 108 - Cumulative loss: 23.46111488342285\n",
      "Epoch: 109 - Cumulative loss: 23.005413055419922\n",
      "Epoch: 110 - Cumulative loss: 22.66046142578125\n",
      "Epoch: 111 - Cumulative loss: 22.346500396728516\n",
      "Epoch: 112 - Cumulative loss: 21.972488403320312\n",
      "Epoch: 113 - Cumulative loss: 21.813175201416016\n",
      "Epoch: 114 - Cumulative loss: 21.78104019165039\n",
      "Epoch: 115 - Cumulative loss: 21.171661376953125\n",
      "Epoch: 116 - Cumulative loss: 21.07761573791504\n",
      "Epoch: 117 - Cumulative loss: 20.730701446533203\n",
      "Epoch: 118 - Cumulative loss: 20.328899383544922\n",
      "Epoch: 119 - Cumulative loss: 19.968807220458984\n",
      "Epoch: 120 - Cumulative loss: 20.127527236938477\n",
      "Epoch: 121 - Cumulative loss: 19.540302276611328\n",
      "Epoch: 122 - Cumulative loss: 19.473663330078125\n",
      "Epoch: 123 - Cumulative loss: 19.11524200439453\n",
      "Epoch: 124 - Cumulative loss: 19.181201934814453\n",
      "Epoch: 125 - Cumulative loss: 18.55543327331543\n",
      "Epoch: 126 - Cumulative loss: 18.000036239624023\n",
      "Epoch: 127 - Cumulative loss: 18.166719436645508\n",
      "Epoch: 128 - Cumulative loss: 17.94393539428711\n",
      "Epoch: 129 - Cumulative loss: 17.254343032836914\n",
      "Epoch: 130 - Cumulative loss: 17.170927047729492\n",
      "Epoch: 131 - Cumulative loss: 17.0356388092041\n",
      "Epoch: 132 - Cumulative loss: 16.70220184326172\n",
      "Epoch: 133 - Cumulative loss: 15.659196853637695\n",
      "Epoch: 134 - Cumulative loss: 16.072132110595703\n",
      "Epoch: 135 - Cumulative loss: 15.746635437011719\n",
      "Epoch: 136 - Cumulative loss: 15.477090835571289\n",
      "Epoch: 137 - Cumulative loss: 15.115742683410645\n",
      "Epoch: 138 - Cumulative loss: 15.238006591796875\n",
      "Epoch: 139 - Cumulative loss: 15.075582504272461\n",
      "Epoch: 140 - Cumulative loss: 14.609704971313477\n",
      "Epoch: 141 - Cumulative loss: 15.858538627624512\n",
      "Epoch: 142 - Cumulative loss: 14.554952621459961\n",
      "Epoch: 143 - Cumulative loss: 14.047103881835938\n",
      "Epoch: 144 - Cumulative loss: 13.932294845581055\n",
      "Epoch: 145 - Cumulative loss: 13.454272270202637\n",
      "Epoch: 146 - Cumulative loss: 13.637228012084961\n",
      "Epoch: 147 - Cumulative loss: 13.310673713684082\n",
      "Epoch: 148 - Cumulative loss: 12.829061508178711\n",
      "Epoch: 149 - Cumulative loss: 13.273449897766113\n",
      "Epoch: 150 - Cumulative loss: 12.623820304870605\n",
      "Epoch: 151 - Cumulative loss: 12.305342674255371\n",
      "Epoch: 152 - Cumulative loss: 12.108023643493652\n",
      "Epoch: 153 - Cumulative loss: 12.227580070495605\n",
      "Epoch: 154 - Cumulative loss: 11.741241455078125\n",
      "Epoch: 155 - Cumulative loss: 11.349406242370605\n",
      "Epoch: 156 - Cumulative loss: 11.338348388671875\n",
      "Epoch: 157 - Cumulative loss: 11.103888511657715\n",
      "Epoch: 158 - Cumulative loss: 10.976470947265625\n",
      "Epoch: 159 - Cumulative loss: 10.32852554321289\n",
      "Epoch: 160 - Cumulative loss: 10.217901229858398\n",
      "Epoch: 161 - Cumulative loss: 10.284306526184082\n",
      "Epoch: 162 - Cumulative loss: 9.963211059570312\n",
      "Epoch: 163 - Cumulative loss: 9.990067481994629\n",
      "Epoch: 164 - Cumulative loss: 9.559115409851074\n",
      "Epoch: 165 - Cumulative loss: 9.473797798156738\n",
      "Epoch: 166 - Cumulative loss: 9.352741241455078\n",
      "Epoch: 167 - Cumulative loss: 8.879908561706543\n",
      "Epoch: 168 - Cumulative loss: 9.051460266113281\n",
      "Epoch: 169 - Cumulative loss: 8.77324104309082\n",
      "Epoch: 170 - Cumulative loss: 8.756786346435547\n",
      "Epoch: 171 - Cumulative loss: 8.49597454071045\n",
      "Epoch: 172 - Cumulative loss: 7.912585258483887\n",
      "Epoch: 173 - Cumulative loss: 7.730457305908203\n",
      "Epoch: 174 - Cumulative loss: 7.754572868347168\n",
      "Epoch: 175 - Cumulative loss: 7.40939998626709\n",
      "Epoch: 176 - Cumulative loss: 7.372222900390625\n",
      "Epoch: 177 - Cumulative loss: 7.119052886962891\n",
      "Epoch: 178 - Cumulative loss: 6.8437933921813965\n",
      "Epoch: 179 - Cumulative loss: 6.900307655334473\n",
      "Epoch: 180 - Cumulative loss: 6.774443626403809\n",
      "Epoch: 181 - Cumulative loss: 6.95693826675415\n",
      "Epoch: 182 - Cumulative loss: 6.293270111083984\n",
      "Epoch: 183 - Cumulative loss: 6.021613121032715\n",
      "Epoch: 184 - Cumulative loss: 6.1790971755981445\n",
      "Epoch: 185 - Cumulative loss: 6.213711738586426\n",
      "Epoch: 186 - Cumulative loss: 5.857205867767334\n",
      "Epoch: 187 - Cumulative loss: 5.680113792419434\n",
      "Epoch: 188 - Cumulative loss: 5.500262260437012\n",
      "Epoch: 189 - Cumulative loss: 5.6717000007629395\n",
      "Epoch: 190 - Cumulative loss: 5.203863143920898\n",
      "Epoch: 191 - Cumulative loss: 5.159516334533691\n",
      "Epoch: 192 - Cumulative loss: 4.947295665740967\n",
      "Epoch: 193 - Cumulative loss: 4.723979949951172\n",
      "Epoch: 194 - Cumulative loss: 4.607625961303711\n",
      "Epoch: 195 - Cumulative loss: 4.44580078125\n",
      "Epoch: 196 - Cumulative loss: 4.473114490509033\n",
      "Epoch: 197 - Cumulative loss: 4.211376667022705\n",
      "Epoch: 198 - Cumulative loss: 4.201549530029297\n",
      "Epoch: 199 - Cumulative loss: 4.056394577026367\n",
      "Epoch: 200 - Cumulative loss: 3.929732084274292\n",
      "CPU times: user 9min 36s, sys: 4.01 s, total: 9min 40s\n",
      "Wall time: 50.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    cum_loss = 0\n",
    "    for i, (src_sents, tgt_sents) in enumerate(batch_iter((train_src, train_tgt), batch_size=BATCH_SIZE, shuffle=True)):\n",
    "        optimizer.zero_grad()\n",
    "        batch_size = len(src_sents)\n",
    "\n",
    "        batch_loss = -model(src_sents, tgt_sents).sum()\n",
    "        batch_loss /= batch_size\n",
    "        cum_loss += batch_loss\n",
    "        batch_loss.backward()\n",
    "\n",
    "         # clip gradient\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "    cum_loss /= len(train_src)\n",
    "    print(f\"Epoch: {str(epoch).zfill(3)} - Cumulative loss: {cum_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NMT(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): CharEmbedding(\n",
       "      (char_embed): Embedding(97, 50, padding_idx=0)\n",
       "      (cnn_embed): CharCNNEmbedding(\n",
       "        (conv): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
       "        (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
       "      )\n",
       "      (highway): Highway(\n",
       "        (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (gate): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (encoder): LSTM(256, 256, num_layers=2, bidirectional=True)\n",
       "    (hidden_projection): Linear(in_features=512, out_features=256, bias=False)\n",
       "    (cell_projection): Linear(in_features=512, out_features=256, bias=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): CharEmbedding(\n",
       "      (char_embed): Embedding(97, 50, padding_idx=0)\n",
       "      (cnn_embed): CharCNNEmbedding(\n",
       "        (conv): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
       "        (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
       "      )\n",
       "      (highway): Highway(\n",
       "        (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (gate): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (attention): Attention(\n",
       "      (linear): Linear(in_features=512, out_features=256, bias=False)\n",
       "    )\n",
       "    (decoder): LSTMCell(512, 256)\n",
       "    (combined_projection): Linear(in_features=768, out_features=256, bias=False)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (target_layer): Linear(in_features=256, out_features=32, bias=False)\n",
       "  (char_decoder): CharDecoder(\n",
       "    (embedding): Embedding(97, 50, padding_idx=0)\n",
       "    (char_decoder): LSTM(50, 256)\n",
       "    (linear): Linear(in_features=256, out_features=97, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['Muchas', 'gracias', 'Chris.', 'Y', 'es', 'en', 'verdad', 'un', 'gran', 'honor', 'tener', 'la', 'oportunidad', 'de', 'venir', 'a', 'este', 'escenario', 'por', 'segunda', 'vez.', 'Estoy', 'extremadamente', 'agradecido.']] [24]\n"
     ]
    }
   ],
   "source": [
    "example = [valid_src[0]]\n",
    "example_length = [len(e) for e in example]\n",
    "print(example, example_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_token_tensor = vocab.src.to_tensor(example, tokens=True)\n",
    "example_char_tensor = vocab.src.to_tensor(example, tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([24, 1, 21]), torch.Size([24, 1]))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "example_char_tensor.shape, example_token_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_out, enc_state = model.encoder(example_char_tensor, example_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_state = enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = vocab.tgt.to_tensor([[\"<s>\"]], tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.zeros(1, model.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, dec_state, _ = model.decoder(y_t, enc_out, dec_state, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.target_layer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "vocab.tgt.to_tokens(index)"
   ]
  },
  {
   "source": [
    "## Bringing it all together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_char_tensor = vocab.src.to_tensor(example, tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_out, enc_state = model.encoder(example_char_tensor, example_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"<s>\"\n",
    "out = torch.zeros(1, model.hidden_size)\n",
    "sent = []\n",
    "combined_out = []\n",
    "for i in range(40):\n",
    "    y_t = vocab.tgt.to_tensor([[token]], tokens=False)\n",
    "    out, dec_state, _ = model.decoder(y_t, enc_out, dec_state, out)\n",
    "    combined_out.append(out)\n",
    "    logit = model.target_layer(out)\n",
    "    index = logit.argmax().item()\n",
    "    if index == vocab.tgt.end_token_idx:\n",
    "        break\n",
    "    token = vocab.tgt.to_tokens(index)\n",
    "    sent.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>']"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}