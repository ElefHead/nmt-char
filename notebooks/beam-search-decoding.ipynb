{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from nmt.models import NMT\n",
    "from nmt.datasets import read_corpus, batch_iter, Vocab\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = Path(\"..\") / \"nmt\" / \"datasets\" / \"data\"\n",
    "en_es_data_loc = data_loc / \"en_es_data\"\n",
    "train_data_src_path = en_es_data_loc / \"train_tiny.es\"\n",
    "train_data_tgt_path = en_es_data_loc / \"train_tiny.en\"\n",
    "dev_data_src_path = en_es_data_loc / \"dev_tiny.es\"\n",
    "dev_data_tgt_path = en_es_data_loc / \"dev_tiny.en\"\n",
    "vocab_path = data_loc / \"vocab_tiny_q2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = read_corpus(train_data_src_path)\n",
    "train_tgt = read_corpus(train_data_tgt_path, is_target=True)\n",
    "vocab = Vocab.load(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(26, 32)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "len(vocab.src), len(vocab.tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_src = read_corpus(dev_data_src_path)\n",
    "valid_tgt = read_corpus(dev_data_tgt_path, is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=2\n",
    "MAX_EPOCH=201\n",
    "SEED=42\n",
    "EMBEDDING_SIZE=256\n",
    "HIDDEN_SIZE=256\n",
    "GRAD_CLIP=5.0\n",
    "UNIFORM_INIT=0.1\n",
    "USE_CHAR_DECODER=True\n",
    "LEARNING_RATE=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMT(\n",
    "    vocab=vocab,\n",
    "    embedding_dim=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    use_char_decoder=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "uniformly initialize parameters [-0.100000, +0.100000]\n"
     ]
    }
   ],
   "source": [
    "uniform_init = UNIFORM_INIT\n",
    "if np.abs(uniform_init) > 0.:\n",
    "    print('uniformly initialize parameters [-%f, +%f]' %\n",
    "            (uniform_init, uniform_init), file=sys.stderr)\n",
    "    for p in model.parameters():\n",
    "        p.data.uniform_(-uniform_init, uniform_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 000 - Cumulative loss: 76.59217834472656\n",
      "Epoch: 001 - Cumulative loss: 73.57061767578125\n",
      "Epoch: 002 - Cumulative loss: 70.48408508300781\n",
      "Epoch: 003 - Cumulative loss: 69.05148315429688\n",
      "Epoch: 004 - Cumulative loss: 66.44526672363281\n",
      "Epoch: 005 - Cumulative loss: 63.42730712890625\n",
      "Epoch: 006 - Cumulative loss: 60.61967086791992\n",
      "Epoch: 007 - Cumulative loss: 57.5542106628418\n",
      "Epoch: 008 - Cumulative loss: 54.16157150268555\n",
      "Epoch: 009 - Cumulative loss: 51.2515869140625\n",
      "Epoch: 010 - Cumulative loss: 49.34196853637695\n",
      "Epoch: 011 - Cumulative loss: 48.279747009277344\n",
      "Epoch: 012 - Cumulative loss: 47.420223236083984\n",
      "Epoch: 013 - Cumulative loss: 46.780296325683594\n",
      "Epoch: 014 - Cumulative loss: 45.952064514160156\n",
      "Epoch: 015 - Cumulative loss: 45.76457977294922\n",
      "Epoch: 016 - Cumulative loss: 45.33715057373047\n",
      "Epoch: 017 - Cumulative loss: 45.0976676940918\n",
      "Epoch: 018 - Cumulative loss: 44.86919403076172\n",
      "Epoch: 019 - Cumulative loss: 44.57324981689453\n",
      "Epoch: 020 - Cumulative loss: 44.37232208251953\n",
      "Epoch: 021 - Cumulative loss: 44.136009216308594\n",
      "Epoch: 022 - Cumulative loss: 43.954383850097656\n",
      "Epoch: 023 - Cumulative loss: 43.6412353515625\n",
      "Epoch: 024 - Cumulative loss: 43.482505798339844\n",
      "Epoch: 025 - Cumulative loss: 43.50143051147461\n",
      "Epoch: 026 - Cumulative loss: 43.25385665893555\n",
      "Epoch: 027 - Cumulative loss: 43.145477294921875\n",
      "Epoch: 028 - Cumulative loss: 42.860477447509766\n",
      "Epoch: 029 - Cumulative loss: 42.71823501586914\n",
      "Epoch: 030 - Cumulative loss: 42.738372802734375\n",
      "Epoch: 031 - Cumulative loss: 42.156124114990234\n",
      "Epoch: 032 - Cumulative loss: 42.51057815551758\n",
      "Epoch: 033 - Cumulative loss: 42.507259368896484\n",
      "Epoch: 034 - Cumulative loss: 42.19763946533203\n",
      "Epoch: 035 - Cumulative loss: 41.991188049316406\n",
      "Epoch: 036 - Cumulative loss: 41.79896926879883\n",
      "Epoch: 037 - Cumulative loss: 41.9422492980957\n",
      "Epoch: 038 - Cumulative loss: 41.79326248168945\n",
      "Epoch: 039 - Cumulative loss: 41.88533401489258\n",
      "Epoch: 040 - Cumulative loss: 41.46733474731445\n",
      "Epoch: 041 - Cumulative loss: 41.49586486816406\n",
      "Epoch: 042 - Cumulative loss: 41.17544174194336\n",
      "Epoch: 043 - Cumulative loss: 40.896785736083984\n",
      "Epoch: 044 - Cumulative loss: 40.93334197998047\n",
      "Epoch: 045 - Cumulative loss: 41.04404067993164\n",
      "Epoch: 046 - Cumulative loss: 40.485008239746094\n",
      "Epoch: 047 - Cumulative loss: 40.26819610595703\n",
      "Epoch: 048 - Cumulative loss: 40.37759017944336\n",
      "Epoch: 049 - Cumulative loss: 39.96513748168945\n",
      "Epoch: 050 - Cumulative loss: 40.18136215209961\n",
      "Epoch: 051 - Cumulative loss: 39.88712692260742\n",
      "Epoch: 052 - Cumulative loss: 39.70349884033203\n",
      "Epoch: 053 - Cumulative loss: 39.43894958496094\n",
      "Epoch: 054 - Cumulative loss: 38.948062896728516\n",
      "Epoch: 055 - Cumulative loss: 39.12095260620117\n",
      "Epoch: 056 - Cumulative loss: 38.67359924316406\n",
      "Epoch: 057 - Cumulative loss: 38.59874725341797\n",
      "Epoch: 058 - Cumulative loss: 38.1579704284668\n",
      "Epoch: 059 - Cumulative loss: 38.18653106689453\n",
      "Epoch: 060 - Cumulative loss: 38.045711517333984\n",
      "Epoch: 061 - Cumulative loss: 37.8343391418457\n",
      "Epoch: 062 - Cumulative loss: 36.95966720581055\n",
      "Epoch: 063 - Cumulative loss: 36.83515548706055\n",
      "Epoch: 064 - Cumulative loss: 36.7869758605957\n",
      "Epoch: 065 - Cumulative loss: 36.5568733215332\n",
      "Epoch: 066 - Cumulative loss: 36.448795318603516\n",
      "Epoch: 067 - Cumulative loss: 36.127708435058594\n",
      "Epoch: 068 - Cumulative loss: 35.69292449951172\n",
      "Epoch: 069 - Cumulative loss: 35.320068359375\n",
      "Epoch: 070 - Cumulative loss: 34.81069564819336\n",
      "Epoch: 071 - Cumulative loss: 34.579139709472656\n",
      "Epoch: 072 - Cumulative loss: 34.241573333740234\n",
      "Epoch: 073 - Cumulative loss: 34.23828125\n",
      "Epoch: 074 - Cumulative loss: 33.9043083190918\n",
      "Epoch: 075 - Cumulative loss: 33.47938919067383\n",
      "Epoch: 076 - Cumulative loss: 32.84437561035156\n",
      "Epoch: 077 - Cumulative loss: 32.71349334716797\n",
      "Epoch: 078 - Cumulative loss: 32.707191467285156\n",
      "Epoch: 079 - Cumulative loss: 32.06937026977539\n",
      "Epoch: 080 - Cumulative loss: 31.89510154724121\n",
      "Epoch: 081 - Cumulative loss: 31.221487045288086\n",
      "Epoch: 082 - Cumulative loss: 31.07309913635254\n",
      "Epoch: 083 - Cumulative loss: 30.72830581665039\n",
      "Epoch: 084 - Cumulative loss: 30.434194564819336\n",
      "Epoch: 085 - Cumulative loss: 29.982223510742188\n",
      "Epoch: 086 - Cumulative loss: 29.650243759155273\n",
      "Epoch: 087 - Cumulative loss: 29.279998779296875\n",
      "Epoch: 088 - Cumulative loss: 28.86676025390625\n",
      "Epoch: 089 - Cumulative loss: 28.188007354736328\n",
      "Epoch: 090 - Cumulative loss: 27.957042694091797\n",
      "Epoch: 091 - Cumulative loss: 27.95513343811035\n",
      "Epoch: 092 - Cumulative loss: 26.992645263671875\n",
      "Epoch: 093 - Cumulative loss: 27.0621337890625\n",
      "Epoch: 094 - Cumulative loss: 26.297250747680664\n",
      "Epoch: 095 - Cumulative loss: 26.29732322692871\n",
      "Epoch: 096 - Cumulative loss: 25.727319717407227\n",
      "Epoch: 097 - Cumulative loss: 25.122236251831055\n",
      "Epoch: 098 - Cumulative loss: 25.125524520874023\n",
      "Epoch: 099 - Cumulative loss: 24.807947158813477\n",
      "Epoch: 100 - Cumulative loss: 24.22907257080078\n",
      "Epoch: 101 - Cumulative loss: 23.83502769470215\n",
      "Epoch: 102 - Cumulative loss: 23.799243927001953\n",
      "Epoch: 103 - Cumulative loss: 23.277976989746094\n",
      "Epoch: 104 - Cumulative loss: 22.761581420898438\n",
      "Epoch: 105 - Cumulative loss: 22.079341888427734\n",
      "Epoch: 106 - Cumulative loss: 22.370935440063477\n",
      "Epoch: 107 - Cumulative loss: 22.411169052124023\n",
      "Epoch: 108 - Cumulative loss: 21.706287384033203\n",
      "Epoch: 109 - Cumulative loss: 21.50834083557129\n",
      "Epoch: 110 - Cumulative loss: 21.104549407958984\n",
      "Epoch: 111 - Cumulative loss: 20.361013412475586\n",
      "Epoch: 112 - Cumulative loss: 20.40355682373047\n",
      "Epoch: 113 - Cumulative loss: 19.814645767211914\n",
      "Epoch: 114 - Cumulative loss: 19.902294158935547\n",
      "Epoch: 115 - Cumulative loss: 19.498291015625\n",
      "Epoch: 116 - Cumulative loss: 19.19253921508789\n",
      "Epoch: 117 - Cumulative loss: 18.84541893005371\n",
      "Epoch: 118 - Cumulative loss: 18.501441955566406\n",
      "Epoch: 119 - Cumulative loss: 18.50387191772461\n",
      "Epoch: 120 - Cumulative loss: 18.248245239257812\n",
      "Epoch: 121 - Cumulative loss: 17.817119598388672\n",
      "Epoch: 122 - Cumulative loss: 17.37645721435547\n",
      "Epoch: 123 - Cumulative loss: 17.167301177978516\n",
      "Epoch: 124 - Cumulative loss: 17.172983169555664\n",
      "Epoch: 125 - Cumulative loss: 16.445417404174805\n",
      "Epoch: 126 - Cumulative loss: 16.413318634033203\n",
      "Epoch: 127 - Cumulative loss: 15.99199390411377\n",
      "Epoch: 128 - Cumulative loss: 15.883723258972168\n",
      "Epoch: 129 - Cumulative loss: 15.191474914550781\n",
      "Epoch: 130 - Cumulative loss: 15.050287246704102\n",
      "Epoch: 131 - Cumulative loss: 15.171252250671387\n",
      "Epoch: 132 - Cumulative loss: 14.701924324035645\n",
      "Epoch: 133 - Cumulative loss: 14.139612197875977\n",
      "Epoch: 134 - Cumulative loss: 14.029322624206543\n",
      "Epoch: 135 - Cumulative loss: 13.922863960266113\n",
      "Epoch: 136 - Cumulative loss: 13.718523025512695\n",
      "Epoch: 137 - Cumulative loss: 13.302268981933594\n",
      "Epoch: 138 - Cumulative loss: 13.194424629211426\n",
      "Epoch: 139 - Cumulative loss: 13.205415725708008\n",
      "Epoch: 140 - Cumulative loss: 13.048574447631836\n",
      "Epoch: 141 - Cumulative loss: 12.613897323608398\n",
      "Epoch: 142 - Cumulative loss: 12.48514461517334\n",
      "Epoch: 143 - Cumulative loss: 12.23702335357666\n",
      "Epoch: 144 - Cumulative loss: 12.106131553649902\n",
      "Epoch: 145 - Cumulative loss: 11.592109680175781\n",
      "Epoch: 146 - Cumulative loss: 11.832832336425781\n",
      "Epoch: 147 - Cumulative loss: 11.211051940917969\n",
      "Epoch: 148 - Cumulative loss: 11.085062026977539\n",
      "Epoch: 149 - Cumulative loss: 10.93591594696045\n",
      "Epoch: 150 - Cumulative loss: 10.478403091430664\n",
      "Epoch: 151 - Cumulative loss: 10.950845718383789\n",
      "Epoch: 152 - Cumulative loss: 10.06648063659668\n",
      "Epoch: 153 - Cumulative loss: 10.170756340026855\n",
      "Epoch: 154 - Cumulative loss: 10.871329307556152\n",
      "Epoch: 155 - Cumulative loss: 10.537878036499023\n",
      "Epoch: 156 - Cumulative loss: 9.705625534057617\n",
      "Epoch: 157 - Cumulative loss: 9.889837265014648\n",
      "Epoch: 158 - Cumulative loss: 9.591375350952148\n",
      "Epoch: 159 - Cumulative loss: 8.935144424438477\n",
      "Epoch: 160 - Cumulative loss: 9.096139907836914\n",
      "Epoch: 161 - Cumulative loss: 8.853283882141113\n",
      "Epoch: 162 - Cumulative loss: 8.516164779663086\n",
      "Epoch: 163 - Cumulative loss: 8.338994026184082\n",
      "Epoch: 164 - Cumulative loss: 8.238539695739746\n",
      "Epoch: 165 - Cumulative loss: 8.142870903015137\n",
      "Epoch: 166 - Cumulative loss: 7.897721767425537\n",
      "Epoch: 167 - Cumulative loss: 7.719415187835693\n",
      "Epoch: 168 - Cumulative loss: 7.744653224945068\n",
      "Epoch: 169 - Cumulative loss: 7.26938533782959\n",
      "Epoch: 170 - Cumulative loss: 7.122198581695557\n",
      "Epoch: 171 - Cumulative loss: 6.93053674697876\n",
      "Epoch: 172 - Cumulative loss: 6.792611122131348\n",
      "Epoch: 173 - Cumulative loss: 6.773314476013184\n",
      "Epoch: 174 - Cumulative loss: 6.3142900466918945\n",
      "Epoch: 175 - Cumulative loss: 6.330366134643555\n",
      "Epoch: 176 - Cumulative loss: 6.247359275817871\n",
      "Epoch: 177 - Cumulative loss: 5.981217384338379\n",
      "Epoch: 178 - Cumulative loss: 5.8726677894592285\n",
      "Epoch: 179 - Cumulative loss: 5.425225257873535\n",
      "Epoch: 180 - Cumulative loss: 5.563315391540527\n",
      "Epoch: 181 - Cumulative loss: 5.504690647125244\n",
      "Epoch: 182 - Cumulative loss: 5.1092023849487305\n",
      "Epoch: 183 - Cumulative loss: 5.251162052154541\n",
      "Epoch: 184 - Cumulative loss: 4.830439567565918\n",
      "Epoch: 185 - Cumulative loss: 4.8708295822143555\n",
      "Epoch: 186 - Cumulative loss: 4.720790386199951\n",
      "Epoch: 187 - Cumulative loss: 4.5405497550964355\n",
      "Epoch: 188 - Cumulative loss: 4.422329902648926\n",
      "Epoch: 189 - Cumulative loss: 4.322201728820801\n",
      "Epoch: 190 - Cumulative loss: 4.1234025955200195\n",
      "Epoch: 191 - Cumulative loss: 4.095470428466797\n",
      "Epoch: 192 - Cumulative loss: 3.9211394786834717\n",
      "Epoch: 193 - Cumulative loss: 3.7321338653564453\n",
      "Epoch: 194 - Cumulative loss: 3.866914749145508\n",
      "Epoch: 195 - Cumulative loss: 3.684685468673706\n",
      "Epoch: 196 - Cumulative loss: 3.5353341102600098\n",
      "Epoch: 197 - Cumulative loss: 3.277282238006592\n",
      "Epoch: 198 - Cumulative loss: 3.155264377593994\n",
      "Epoch: 199 - Cumulative loss: 3.135575771331787\n",
      "Epoch: 200 - Cumulative loss: 3.1180262565612793\n",
      "CPU times: user 9min 58s, sys: 2.34 s, total: 10min\n",
      "Wall time: 53.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    cum_loss = 0\n",
    "    for i, (src_sents, tgt_sents) in enumerate(batch_iter((train_src, train_tgt), batch_size=BATCH_SIZE, shuffle=True)):\n",
    "        optimizer.zero_grad()\n",
    "        batch_size = len(src_sents)\n",
    "\n",
    "        batch_loss = -model(src_sents, tgt_sents).sum()\n",
    "        batch_loss /= batch_size\n",
    "        cum_loss += batch_loss\n",
    "        batch_loss.backward()\n",
    "\n",
    "         # clip gradient\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "    cum_loss /= len(train_src)\n",
    "    print(f\"Epoch: {str(epoch).zfill(3)} - Cumulative loss: {cum_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}