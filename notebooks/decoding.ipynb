{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from nmt.datasets import Vocab, batch_iter\n",
    "from nmt.networks import CharEmbedding, Encoder\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup something to work with\n",
    "\n",
    "sentences_words_src = [\n",
    "    ['Human:', 'What', 'do', 'we', 'want?'],\n",
    "    ['Computer:', 'Natural', 'language', 'processing!'],\n",
    "    ['Human:', 'When', 'do', 'we', 'want', 'it?'],\n",
    "    ['Computer:', 'When', 'do', 'we', 'want', 'what?']\n",
    "]\n",
    "\n",
    "sentences_words_tgt = [\n",
    "    ['<s>', 'Human:', 'What', 'do', 'we', 'want?', '</s>'],\n",
    "    ['<s>', 'Computer:', 'Natural', 'language', 'processing!', '</s>'],\n",
    "    ['<s>', 'Human:', 'When', 'do', 'we', 'want', 'it?', '</s>'],\n",
    "    ['<s>', 'Computer:', 'When', 'do', 'we', 'want', 'what?', '</s>']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing source vocab\nVocab Store: Tokens [size=17],                 Characters [size=97]\nInitializing target vocab\nVocab Store: Tokens [size=17],                 Characters [size=97]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab.build(sentences_words_src, sentences_words_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['Human:', 'When', 'do', 'we', 'want', 'it?'], ['Computer:', 'When', 'do', 'we', 'want', 'what?'], ['Human:', 'What', 'do', 'we', 'want?'], ['Computer:', 'Natural', 'language', 'processing!']]\n"
     ]
    }
   ],
   "source": [
    "data = list(zip(sentences_words_src, sentences_words_tgt))\n",
    "data_generator = batch_iter(\n",
    "    data=data,\n",
    "    batch_size=4,\n",
    "    shuffle=True\n",
    ")\n",
    "batch_src, batch_tgt = next(data_generator)\n",
    "print(batch_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[6, 6, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "source_length = [len(sent) for sent in batch_src]\n",
    "print(source_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "src char tensor size = torch.Size([6, 4, 21]); tgt char tensor size = torch.Size([8, 4, 21])\n"
     ]
    }
   ],
   "source": [
    "char_tensors_src = vocab.src.to_tensor(batch_src, tokens=False)\n",
    "token_tensors_tgt = vocab.tgt.to_tensor(batch_tgt, tokens=False)\n",
    "print(f\"src char tensor size = {char_tensors.size()}; tgt char tensor size = {token_tensors_tgt.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_size=300, hidden_size=1024, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([6, 4, 300]) torch.Size([8, 4, 300])\n"
     ]
    }
   ],
   "source": [
    "c_embedding = CharEmbedding(num_embeddings=vocab.src.length(tokens=False), char_embedding_dim=50, embedding_dim=300, char_padding_idx=vocab.src.pad_char_idx)\n",
    "char_embedding_src = c_embedding(char_tensors_src)\n",
    "t_embedding = CharEmbedding(num_embeddings=vocab.tgt.length(tokens=False), char_embedding_dim=50, embedding_dim=300, char_padding_idx=vocab.tgt.pad_char_idx)\n",
    "target_embedding = t_embedding(token_tensors_tgt)\n",
    "print(char_embedding.size(), target_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([4, 6, 2048]), torch.Size([4, 1024]), torch.Size([4, 1024]))"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "char_enc_hidden, (char_hidden, char_cell) = encoder(char_embedding_src, source_length)\n",
    "char_enc_hidden.shape, char_hidden.shape, char_cell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def forward(self, enc_hidden: torch.Tensor,\n",
    "                enc_projection: torch.Tensor,\n",
    "                dec_hidden_t: torch.Tensor,\n",
    "                enc_masks: torch.Tensor = None) -> torch.Tensor:\n",
    "        dec_hidden_unsqueezed_t = dec_hidden_t.unsqueeze(dim=2)\n",
    "        score_t = enc_projection.bmm(dec_hidden_unsqueezed_t)\n",
    "        score_t = score_t.squeeze(dim=2)\n",
    "\n",
    "        if enc_masks:\n",
    "            score_t.data.masked_fill_(\n",
    "                enc_masks.byte().to(torch.bool),\n",
    "                -float('inf')\n",
    "            )\n",
    "        \n",
    "        alpha_t = F.softmax(score_t, dim=1)\n",
    "        alpha_t = alpha_t.unsqueeze(dim=1)\n",
    "\n",
    "        attention = alpha_t.bmm(enc_hidden)\n",
    "        return attention.squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = Attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiplications with hidden layers inside attention layer\n",
    "encoder_proj = nn.Linear(in_features=2048, out_features=1024, bias=False)\n",
    "enc_projection = encoder_proj(char_enc_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 2048])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "attention(char_enc_hidden, enc_projection, char_hidden).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size: int,\n",
    "                 hidden_size: int, \n",
    "                 bias: bool = True,\n",
    "                 dropout_prob: float = 0.3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention = Attention()\n",
    "        self.decoder = nn.LSTMCell(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            bias=bias\n",
    "        )\n",
    "        self.combined_projection = nn.Linear(\n",
    "            in_features=hidden_size*3,\n",
    "            out_features=hidden_size,\n",
    "            bias=False\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "    def forward(self, output: torch.Tensor,\n",
    "                enc_hidden: torch.Tensor,\n",
    "                enc_projection: torch.Tensor,\n",
    "                dec_init_state: Tuple[torch.Tensor, torch.Tensor],\n",
    "                enc_masks: torch.Tensor = None,\n",
    "                device: torch.device = 'cpu') -> torch.Tensor:\n",
    "        \n",
    "        dec_state = dec_init_state\n",
    "        batch_size, sent_length, _ = enc_hidden.size()\n",
    "\n",
    "        o_prev = torch.zeros(batch_size, self.hidden_size, device=device)\n",
    "        combined_outputs = []\n",
    "\n",
    "        for Y_t in torch.split(output, 1, dim=0):\n",
    "            Ybar_t = torch.cat([Y_t.squeeze(dim=0), o_prev], dim=1)\n",
    "\n",
    "            dec_state = self.decoder(Ybar_t, dec_state)\n",
    "            dec_hidden, dec_cell = dec_state\n",
    "\n",
    "            a_t = self.attention(enc_hidden, enc_projection, dec_hidden, enc_masks)\n",
    "\n",
    "            U_t = torch.cat([a_t, dec_hidden], dim=1)\n",
    "            V_t = self.combined_projection(U_t)\n",
    "            o_t = self.dropout(V_t.tanh())\n",
    "\n",
    "            combined_outputs.append(o_t)\n",
    "            o_prev = o_t\n",
    "\n",
    "        combined_outputs = torch.stack(combined_outputs, dim=0)\n",
    "        return combined_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(\n",
    "    input_size=1024+300,\n",
    "    hidden_size=1024\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embedding = target_embedding[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 300])"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "target_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = decoder(target_embedding, char_enc_hidden, enc_projection, (char_hidden, char_cell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = nn.Linear(\n",
    "            in_features=1024,\n",
    "            out_features=len(vocab.tgt),\n",
    "            bias=False\n",
    "        )\n",
    "P = F.log_softmax(target_layer(outputs), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 17])"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}