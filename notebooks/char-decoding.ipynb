{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "source": [
    "From Luong and Manning (2016)\n",
    "> The main idea is that when our word-level decoder produces an <UNK\\> token, we run our character-level decoder (which you can think of as a character-level conditional language model) to instead generate the target word one character at a time.\n",
    "\n",
    "<img src=\"images/char-nmt-luong-manning.png\" />   \n",
    "\n",
    "-- From assignment 5 --  \n",
    "We have to deal with three parts of the character level decoder:  \n",
    "\n",
    "> When we train the NMT system, we train the character decoder on every word in the target sentence (not just the words represented by <UNK\\>).\n",
    "\n",
    "1. **Forward pass**: \n",
    "    * Get input character sequences - convert to character embeddings.\n",
    "    * Run through unidirectional LSTM to get hidden states and cell states.\n",
    "        * The initial hidden states are set to combined output from decoder. \n",
    "    * For each timestep, compute logits (using char_decoder W and char_decoder bias). Logits dimension `d = vocab.tgt.length(tokens=False)`.\n",
    "2. **Forward pass during training**:  \n",
    "    * We do a forward pass with an input character sequence and get logits\n",
    "    * Compare with target sequence and minimize using cross-entropy loss. Sum it up for a batch. \n",
    "    * Add loss of to word-based decoder loss   \n",
    "3. **During Test: Greedy decoding**:\n",
    "    * During test time - first produce translation from NMT token based system.\n",
    "    * *IF* the translation contains an _<unk>_ then for those positions we use the character decpder using the `combined_output` from decoder to initialize.\n",
    "    * Use greedy decoding algorithm.  \n",
    "\n",
    "<img src=\"images/greedy_decoding.png\">\n",
    "\n",
    "Note: To maintain the design pattern that I have been following, I will defer the implementation of steps 2 and 3 in main NMT forward. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDecoder(nn.Module):\n",
    "    def __init__(self, num_embeddings: int,\n",
    "                 hidden_size: int, padding_idx: int,\n",
    "                 embedding_dim: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the character level decoder\n",
    "        Check notebooks: char-decoding.ipynb for explanation\n",
    "        @param num_embeddings: number of embeddings.\n",
    "            It equals vocab.tgt.length(tokens=False).\n",
    "        @param hidden_size: hidden units for lstm layer.\n",
    "        @param padding_idx: target char idx.\n",
    "        @param embedding_dim: embedding dimension.\n",
    "        \"\"\"\n",
    "        super(CharDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=num_embeddings,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        self.char_decoder = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=hidden_size,\n",
    "            out_features=num_embeddings\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                dec_init: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward computation for char decoder\n",
    "        @param x: character embedding tensor.\n",
    "        @param dec_init: combined_output tensor from decoder network.\n",
    "\n",
    "        @returns score: tensor of logits\n",
    "        @returns dec_state: lstm hidden and cell states\n",
    "        \"\"\"\n",
    "        dec_state = dec_init\n",
    "        char_embedding = self.embedding(x)\n",
    "        output, dec_state = self.char_decoder(\n",
    "            char_embedding,\n",
    "            dec_state\n",
    "        )\n",
    "        score = self.linear(output)\n",
    "        return score, dec_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}