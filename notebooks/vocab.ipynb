{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Dict, Union, Tuple\n",
    "from functools import reduce\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup small tests - borrowed from CS224N 2018-19 Homework 5\n",
    "sentences_words = [['Human:', 'What', 'do', 'we', 'want?'], ['Computer:', 'Natural', 'language', 'processing!'], [\n",
    "        'Human:', 'When', 'do', 'we', 'want', 'it?'], ['Computer:', 'When', 'do', 'we', 'want', 'what?']]"
   ]
  },
  {
   "source": [
    "*Create class for source or target vocabulary and another that essentially contains variables to store both*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Borrowed from d2l.ai http://d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html#vocabulary\n",
    "\n",
    "def count_corpus(tokens: Union[List[str], List[List[str]]],\n",
    "                 sort: bool = True) -> List[tuple]:\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens for token in line] ## Flatten 2D List into 1D list\n",
    "    sorted_counts = sorted(Counter(tokens).items(), key=lambda x: x[0]) ## By ascii\n",
    "    sorted_counts = sorted(sorted_counts, key=lambda x: x[1], reverse=True) ## By count descending\n",
    "    return sorted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('do', 3), ('we', 3), ('Computer:', 2), ('Human:', 2), ('When', 2), ('want', 2), ('Natural', 1), ('What', 1), ('it?', 1), ('language', 1), ('processing!', 1), ('want?', 1), ('what?', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(count_corpus(sentences_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sents(sents: List[List[int]], pad_token: int) -> List[List[int]]:\n",
    "    sents_padded = []\n",
    "\n",
    "    max_sent_length = reduce(max, map(len, sents))\n",
    "    for sent in sents:\n",
    "        sents_padded.append(sent + [pad_token]*(max_sent_length - len(sent)))\n",
    "    \n",
    "    return sents_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sents_char(sents: List[List[List[int]]], pad_token: int, max_word_length: int = 21) -> List[List[List[int]]]:\n",
    "    max_sent_length = reduce(max, map(len, sents))\n",
    "    empty_word = [pad_token] * max_word_length\n",
    "    sents_padded = []\n",
    "    for sent in sents:\n",
    "        sent_padded = [(word + [pad_token]*(max_word_length - len(word)))[:max_word_length] for word in sent]\n",
    "        sents_padded.append(sent_padded + [empty_word] * (max_sent_length - len(sent)))\n",
    "    return sents_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mix of d2l.ai http://d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html#vocabulary\n",
    "## and CS224N 2018-19 Homework 5\n",
    "## everything with modifications\n",
    "\n",
    "class VocabStore(object):\n",
    "    \"\"\"\n",
    "    Will store source or target vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens: List[List[str]] = None,\n",
    "                 token2id: Dict[str, int] = None,\n",
    "                 min_freq: int = 0,\n",
    "                 reserved_tokens: Dict[str, str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Constructs the vocabulary\n",
    "\n",
    "        @param tokens: List of tokenized sentences\n",
    "        @param token2id: Dictionary of token to id mapping when loading\n",
    "            from saved json vocabstore.\n",
    "        @param min_freq: Int(default = 0) discard threshold for rare words\n",
    "        @param reserved_tokens: Dict of start, end, pad, unk tokens. Key is\n",
    "            the name and value is the token. Eg. {'start': '<s>'}\n",
    "        \"\"\"\n",
    "        # For handling tokens\n",
    "        if token2id:  # restore from save\n",
    "            self.token2id = token2id\n",
    "            self.start_token = token2id[\"<s>\"]\n",
    "            self.end_token = token2id[\"</s>\"]\n",
    "            self.unk = token2id[\"<unk>\"]\n",
    "            self.pad = token2id[\"<pad>\"]\n",
    "\n",
    "            self.id2word = {v: k for k, v in token2id.items()}\n",
    "\n",
    "        else:  # build new\n",
    "            self.token2id = {}\n",
    "            if not reserved_tokens:\n",
    "                reserved_tokens = {}\n",
    "\n",
    "            reserved_tokens[\"unk\"] = reserved_tokens.get(\"unk\", \"<unk>\")\n",
    "            reserved_tokens[\"pad\"] = reserved_tokens.get(\"pad\", \"<pad>\")\n",
    "            reserved_tokens[\"start\"] = reserved_tokens.get(\"start\", \"<s>\")\n",
    "            reserved_tokens[\"end\"] = reserved_tokens.get(\"end\", \"</s>\")\n",
    "\n",
    "            self.start_token, self.token2id[reserved_tokens['start']\n",
    "                                            ] = reserved_tokens[\"start\"], 1\n",
    "            self.end_token, self.token2id[reserved_tokens['end']\n",
    "                                          ] = reserved_tokens[\"end\"], 2\n",
    "            self.unk, self.token2id[reserved_tokens['unk']\n",
    "                                    ] = reserved_tokens[\"unk\"], 3\n",
    "            self.pad, self.token2id[reserved_tokens['pad']\n",
    "                                    ] = reserved_tokens[\"pad\"], 0\n",
    "\n",
    "            if not tokens:\n",
    "                tokens = []\n",
    "\n",
    "            self.id2word = {}\n",
    "            uniq_tokens = list(self.token2id.keys())\n",
    "            token_freqs = count_corpus(tokens)\n",
    "            uniq_tokens += [token for token, freq in token_freqs\n",
    "                            if freq >= min_freq and token not in uniq_tokens]\n",
    "\n",
    "            for token in uniq_tokens:\n",
    "                self.token2id[token] = self.token2id.get(\n",
    "                    token, len(self.token2id))\n",
    "                self.id2word[self.token2id[token]] = token\n",
    "\n",
    "        # For handling chars\n",
    "\n",
    "        self.char_list = list(\n",
    "            \"\"\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;\n",
    "            .!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]\"\"\".replace(\"\\n\", \"\")\n",
    "        )\n",
    "        self.char2id = {}\n",
    "        self.char2id[self.pad] = 0\n",
    "        self.start_char, self.char2id[\"{\"] = \"{\", 1\n",
    "        self.end_char, self.char2id[\"}\"] = \"}\", 2\n",
    "        self.char2id[self.unk] = 3\n",
    "\n",
    "        for c in self.char_list:\n",
    "            self.char2id[c] = self.char2id.get(c, len(self.char2id))\n",
    "\n",
    "        self.id2char = {v: k for k, v in self.char2id.items()}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Overrides len() function.\n",
    "        @returns int: number of tokens in vocabulary\n",
    "        \"\"\"\n",
    "        return len(self.token2id)\n",
    "\n",
    "    def __getitem__(self, tokens: Union[List[str], Tuple[str], str]) -> \\\n",
    "            Union[List[int], int]:\n",
    "        \"\"\"\n",
    "        Retrieves token's index. Can handle a list of tokens\n",
    "            or just a single token.\n",
    "        @param tokens: List, tuple of tokens or single token\n",
    "        @returns list or int: if input was a container then a list of indices\n",
    "            otherwise a single index. Unk index if token not in vocab.\n",
    "        \"\"\"\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token2id.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def __contains__(self, token) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if token is in vocabulary.\n",
    "        @param token (str): token to look up.\n",
    "        @returns bool: True if token in vocab else False\n",
    "        \"\"\"\n",
    "        return token in self.token2id\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\"\n",
    "        Raises error; Vocab is read only\n",
    "        \"\"\"\n",
    "        raise ValueError(\"Vocabulary store is read only\")\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        Sets print representation\n",
    "        \"\"\"\n",
    "        return f\"\"\"Vocab Store: Tokens [size={len(self)}],\n",
    "                Characters [size={len(self.char2id)}]\"\"\".replace(\"\\n\", \" \")\n",
    "\n",
    "    def to_tokens(self, indices: Union[List[int], Tuple[int], int]) -> \\\n",
    "            Union[List[str], str]:\n",
    "        \"\"\"\n",
    "        Converts indices to tokens\n",
    "        @param indices: List of indices or a single index\n",
    "        @returns list of corresponding tokens if indices is a list\n",
    "            else a single token\n",
    "        \"\"\"\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.id2word.get(indices, None)\n",
    "        return [self.to_tokens(index) for index in indices]\n",
    "\n",
    "    def length(self, tokens: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Another function for computing length of vocab for handling\n",
    "        tokens as well as characters.\n",
    "        @param tokens: bool (default = True) if set to true\n",
    "            it will return same as len(vocab) - number of tokens in vocab\n",
    "            if set to false, it will return number of characters in vocab\n",
    "        @retuns int: number of tokens or number of characters in vocab\n",
    "        \"\"\"\n",
    "        return len(self.token2id) if tokens else len(self.char2id)\n",
    "\n",
    "    def add(self, token: str) -> int:\n",
    "        \"\"\"\n",
    "        Add token to vocab if previouslu unseen.\n",
    "        @param token: token string to be added\n",
    "        @returns index of given token\n",
    "        \"\"\"\n",
    "        self.token2id[token] = self.token2id.get(token, len(self))\n",
    "        idx = self.token2id[token]\n",
    "        self.id2word[idx] = token\n",
    "        return idx\n",
    "\n",
    "    def sent2id(self, sents: List[List[str]]) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Converts a sentence (list of list of token string) to\n",
    "        corresponding indices (list of list of index int)\n",
    "        @params: sents : input sentences as list of list of token strings\n",
    "        @returns: corresponding indices (list of list of ints)\n",
    "        \"\"\"\n",
    "        return [self[sent] for sent in sents]\n",
    "\n",
    "    def to_charid(self, char: Union[List[str], str]) -> Union[List[int], int]:\n",
    "        \"\"\"\n",
    "        Converts chars to corresponding char indices\n",
    "        @params char: one character or list of characters\n",
    "        @returns list of corresponding char indices if input is list\n",
    "            else single char index\n",
    "        \"\"\"\n",
    "        if not isinstance(char, (list, tuple)):\n",
    "            return self.char2id.get(char, self.unk)\n",
    "        return [self.to_charid(c) for c in char]\n",
    "\n",
    "    def word2char(self, tokens: Union[List[str], str]) -> \\\n",
    "            Union[List[List[int]], List[int]]:\n",
    "        \"\"\"\n",
    "        Converts token(s) to corresponding char indices\n",
    "        @param tokens: single token or a list of tokens\n",
    "        @returns list of indices with start of word and\n",
    "            end of word char indices appended.\n",
    "        \"\"\"\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return [self.char2id.get(char, self.unk)\n",
    "                    for char in self.start_char + tokens + self.end_char]\n",
    "        return [self.word2char(token) for token in tokens]\n",
    "\n",
    "    def to_char(self, indices: Union[int, List[int]]) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Converts indices to corresponding chars.\n",
    "        @param indices: single index or list of indices\n",
    "        @returns single char or a list of chars.\n",
    "        \"\"\"\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.id2char.get(indices, None)\n",
    "        return [self.to_char(index) for index in indices]\n",
    "\n",
    "    def sent2charid(self, sents: List[List[str]]) -> List[List[List[int]]]:\n",
    "        \"\"\"\n",
    "        Converts sentences to corresponding char indices\n",
    "        @param sents: list of list of token strings\n",
    "        @returns indices: list of list of list of char indices\n",
    "        \"\"\"\n",
    "        return [self.word2char(sent) for sent in sents]\n",
    "\n",
    "    def to_tensor(self, sents: List[List[str]],\n",
    "                  tokens: bool,\n",
    "                  device: torch.device = 'cpu',\n",
    "                  max_word_length: int = 21) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Converts sentences to token or char index tensors\n",
    "        @param sents: list of list of token strings.\n",
    "        @param tokens: bool representing requirement of token\n",
    "            tensor or character tensor. Set true for token tensors.\n",
    "            Set false for character tensors.\n",
    "        @param device (default: \"cpu\"): cpu or gpu for loading the tensor.\n",
    "        @max_word_length (default: 21) : maximum allowed length of words.\n",
    "            any longer word will be truncated. shorter words will get padded.\n",
    "\n",
    "        @returns token tensor : torch.Tensor of\n",
    "            shape (sentence_length, batch_size) if tokens true\n",
    "            else char tensor: torch.Tensor of\n",
    "            shape (sentence_length, batch_size, max_word_length)\n",
    "        \"\"\"\n",
    "        ids = self.sent2id(sents) if tokens else self.sent2charid(sents)\n",
    "        pad_ids = pad_sents(ids, self[self.pad]) if tokens else pad_sents_char(\n",
    "            ids, self.to_charid(self.pad), max_word_length=max_word_length)\n",
    "        tensor_sents = torch.tensor(pad_ids, dtype=torch.long, device=device)\n",
    "        return torch.t(tensor_sents) if tokens \\\n",
    "            else tensor_sents.permute([1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab Store: Tokens [size=17],                 Characters [size=97]\n[('<s>', 1), ('</s>', 2), ('<unk>', 3), ('<pad>', 0), ('do', 4), ('we', 5), ('Computer:', 6), ('Human:', 7), ('When', 8), ('want', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab_store = VocabStore(sentences_words)\n",
    "print(vocab_store)\n",
    "print(list(vocab_store.token2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nFalse\n"
     ]
    }
   ],
   "source": [
    "print(\"we\" in vocab_store)\n",
    "print(\"NOTWE\" in vocab_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[7, 11, 4, 5, 15]\n"
     ]
    }
   ],
   "source": [
    "print(vocab_store[sentences_words[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Human:', 'What', 'do', 'we', 'want?']\n"
     ]
    }
   ],
   "source": [
    "print(vocab_store.to_tokens([7, 11, 4, 5, 15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[7, 11, 4, 5, 15], [6, 10, 13, 14], [7, 8, 4, 5, 9, 12], [6, 8, 4, 5, 9, 16]]"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "vocab_store.sent2id(sentences_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[7, 11, 4, 5, 15, 0],\n",
       " [6, 10, 13, 14, 0, 0],\n",
       " [7, 8, 4, 5, 9, 12],\n",
       " [6, 8, 4, 5, 9, 16]]"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "pad_sents(vocab_store.sent2id(sentences_words), vocab_store[vocab_store.pad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Human:', 'What', 'do', 'we', 'want?']"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "vocab_store.to_tokens([7, 11, 4, 5, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[1, 11, 50, 42, 30, 43, 72, 2],\n",
       " [1, 26, 37, 30, 49, 2],\n",
       " [1, 33, 44, 2],\n",
       " [1, 52, 34, 2],\n",
       " [1, 52, 30, 43, 49, 71, 2]]"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "vocab_store.word2char(sentences_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['{', 'G', 't', 'l', 'Z', 'm', '!', '}']"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "vocab_store.to_char([1, 10, 49, 41, 29, 42, 70, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[[1, 11, 50, 42, 30, 43, 72, 2],\n",
       "  [1, 26, 37, 30, 49, 2],\n",
       "  [1, 33, 44, 2],\n",
       "  [1, 52, 34, 2],\n",
       "  [1, 52, 30, 43, 49, 71, 2]],\n",
       " [[1, 6, 44, 42, 45, 50, 49, 34, 47, 72, 2],\n",
       "  [1, 17, 30, 49, 50, 47, 30, 41, 2],\n",
       "  [1, 41, 30, 43, 36, 50, 30, 36, 34, 2],\n",
       "  [1, 45, 47, 44, 32, 34, 48, 48, 38, 43, 36, 70, 2]],\n",
       " [[1, 11, 50, 42, 30, 43, 72, 2],\n",
       "  [1, 26, 37, 34, 43, 2],\n",
       "  [1, 33, 44, 2],\n",
       "  [1, 52, 34, 2],\n",
       "  [1, 52, 30, 43, 49, 2],\n",
       "  [1, 38, 49, 71, 2]],\n",
       " [[1, 6, 44, 42, 45, 50, 49, 34, 47, 72, 2],\n",
       "  [1, 26, 37, 34, 43, 2],\n",
       "  [1, 33, 44, 2],\n",
       "  [1, 52, 34, 2],\n",
       "  [1, 52, 30, 43, 49, 2],\n",
       "  [1, 52, 37, 30, 49, 71, 2]]]"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "vocab_store.sent2charid(sentences_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[[1, 11, 50, 42, 30, 43, 72, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 26, 37, 30, 49, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 33, 44, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 34, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 30, 43, 49, 71, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [[1, 6, 44, 42, 45, 50, 49, 34, 47, 72, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 17, 30, 49, 50, 47, 30, 41, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 41, 30, 43, 36, 50, 30, 36, 34, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 45, 47, 44, 32, 34, 48, 48, 38, 43, 36, 70, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [[1, 11, 50, 42, 30, 43, 72, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 26, 37, 34, 43, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 33, 44, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 34, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 30, 43, 49, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 38, 49, 71, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [[1, 6, 44, 42, 45, 50, 49, 34, 47, 72, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 26, 37, 34, 43, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 33, 44, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 34, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 30, 43, 49, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 37, 30, 49, 71, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "pad_sents_char(vocab_store.sent2charid(sentences_words), vocab_store.to_charid(vocab_store.pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([6, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "vocab_store.to_tensor(sentences_words, tokens=True, device=\"cpu\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([6, 4, 21])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "vocab_store.to_tensor(sentences_words, tokens=False, device=\"cpu\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Borrowed from CS224N 2018-19 Homework 5 with modifications\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, src_vocab: VocabStore = None,\n",
    "                 tgt_vocab: VocabStore = None) -> None:\n",
    "        \"\"\"\n",
    "        Create vocabulary for NMT task.\n",
    "\n",
    "        @param src_vocab (VocabStore): VocabStore for source language\n",
    "        @param tgt_vocab (VocabStore): VocabStore for target language\n",
    "        \"\"\"\n",
    "        self.src = src_vocab\n",
    "        self.tgt = tgt_vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def build(src_sents: Union[List[str], List[List[str]]],\n",
    "              tgt_sents: Union[List[str], List[List[str]]],\n",
    "              min_freq: int = 0) -> 'Vocab':\n",
    "        \"\"\"\n",
    "        Build Vocabulary for NMT task.\n",
    "\n",
    "        @param src_sents: Source sentences\n",
    "        @param tgt_sents: Target sentences\n",
    "        @param min_freq (int): if token occurs n < freq_cutoff times, drop it.\n",
    "\n",
    "        @returns vocab object containing source and target vocabulary.\n",
    "        \"\"\"\n",
    "        if src_sents and isinstance(src_sents[0], str):\n",
    "            src_sents = [line.split() for line in src_sents]\n",
    "\n",
    "        if tgt_sents and isinstance(tgt_sents[0], str):\n",
    "            tgt_sents = [line.split() for line in tgt_sents]\n",
    "\n",
    "        assert len(src_sents) == len(tgt_sents)\n",
    "\n",
    "        print(\"Initializing source vocab\")\n",
    "        src = VocabStore(src_sents, min_freq=min_freq)\n",
    "        print(src)\n",
    "\n",
    "        print(\"Initializing target vocab\")\n",
    "        tgt = VocabStore(tgt_sents, min_freq=min_freq)\n",
    "        print(tgt)\n",
    "\n",
    "        return Vocab(src, tgt)\n",
    "\n",
    "    def save(self, filepath: Union[Path, str]):\n",
    "        \"\"\"\n",
    "        Save Vocab to file as JSON dump.\n",
    "        @param filepath (str, Path): file path to vocab file\n",
    "        \"\"\"\n",
    "        if isinstance(filepath, str):\n",
    "            filepath = Path(filepath)\n",
    "\n",
    "        if not filepath.parent.is_dir():\n",
    "            filepath.parent.mkdir(parents=True)\n",
    "\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(json.dumps(dict(src_token2id=self.src.token2id,\n",
    "                                    tgt_token2id=self.tgt.token2id)))\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath: Union[Path, str]) -> 'Vocab':\n",
    "        \"\"\"\n",
    "        Load vocabulary from JSON dump.\n",
    "        @param filepath (str, Path): file path to vocab file\n",
    "        @returns Vocab object loaded from JSON dump\n",
    "        \"\"\"\n",
    "        vocab_data = {}\n",
    "        with open(filepath, \"r\") as f:\n",
    "            vocab_data = json.loads(f.read())\n",
    "\n",
    "        src_token2id = vocab_data[\"src_token2id\"]\n",
    "        tgt_token2id = vocab_data[\"tgt_token2id\"]\n",
    "\n",
    "        return Vocab(\n",
    "            VocabStore(token2id=src_token2id),\n",
    "            VocabStore(token2id=tgt_token2id)\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Representation of Vocab to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return f'Vocab src: {self.src}, tgt: {self.tgt}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing source vocab\nVocab Store: Tokens [size=17],                 Characters [size=97]\nInitializing target vocab\nVocab Store: Tokens [size=17],                 Characters [size=97]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab.build(sentences_words, sentences_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.save(\"../nmt/datasets/data/test_data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2 = Vocab.load(\"../nmt/datasets/data/test_data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab src: Vocab Store: Tokens [size=17],                 Characters [size=97], tgt: Vocab Store: Tokens [size=17],                 Characters [size=97]\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab src: Vocab Store: Tokens [size=17],                 Characters [size=97], tgt: Vocab Store: Tokens [size=17],                 Characters [size=97]\n"
     ]
    }
   ],
   "source": [
    "print(vocab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}