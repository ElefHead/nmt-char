{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Dict, Union, Tuple\n",
    "from functools import reduce\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup small tests - borrowed from CS224N 2018-19 Homework 5\n",
    "sentences_small = [[\"a\", \"b\", \"c?\"], [\"~d~\", \"c\", \"b\", \"a\"]]\n",
    "sentences_words = [['Human:', 'What', 'do', 'we', 'want?'], ['Computer:', 'Natural', 'language', 'processing!'], [\n",
    "        'Human:', 'When', 'do', 'we', 'want', 'it?'], ['Computer:', 'When', 'do', 'we', 'want', 'what?']]"
   ]
  },
  {
   "source": [
    "*Create class for source or target vocabulary and another that essentially contains variables to store both*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Borrowed from d2l.ai http://d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html#vocabulary\n",
    "\n",
    "def count_corpus(tokens: Union[List[str], List[List[str]]],\n",
    "                 sort: bool = True) -> List[tuple]:\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens for token in line] ## Flatten 2D List into 1D list\n",
    "    sorted_counts = sorted(Counter(tokens).items(), key=lambda x: x[0]) ## By ascii\n",
    "    sorted_counts = sorted(sorted_counts, key=lambda x: x[1], reverse=True) ## By count descending\n",
    "    return sorted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('do', 3), ('we', 3), ('Computer:', 2), ('Human:', 2), ('When', 2), ('want', 2), ('Natural', 1), ('What', 1), ('it?', 1), ('language', 1), ('processing!', 1), ('want?', 1), ('what?', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(count_corpus(sentences_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sents(sents: List[List[int]], pad_token: int) -> List[List[int]]:\n",
    "    sents_padded = []\n",
    "\n",
    "    max_sent_length = reduce(max, map(len, sents))\n",
    "    for sent in sents:\n",
    "        sents_padded.append(sent + [pad_token]*(max_sent_length - len(sent)))\n",
    "    \n",
    "    return sents_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sents_char(sents: List[List[List[int]]], pad_token: int, max_word_length: int = 21) -> List[List[List[int]]]:\n",
    "    max_sent_length = reduce(max, map(len, sents))\n",
    "    empty_word = [pad_token] * max_word_length\n",
    "    sents_padded = []\n",
    "    for sent in sents:\n",
    "        sent_padded = [(word + [pad_token]*(max_word_length - len(word)))[:max_word_length] for word in sent]\n",
    "        sents_padded.append(sent_padded + [empty_word] * (max_sent_length - len(sent)))\n",
    "    return sents_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabStore(object):\n",
    "    \"\"\"\n",
    "    Will store source or target vocabulary\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens: List[List[str]] = None, token2id: Dict[str, int] = None, \n",
    "                 min_freq: int = 0, reserved_tokens: Dict[str, str] = None) -> None:\n",
    "\n",
    "        # For handling tokens\n",
    "\n",
    "        if token2id:\n",
    "            self.token2id = token2id\n",
    "\n",
    "        else:\n",
    "            self.token2id = {}\n",
    "            if not reserved_tokens:\n",
    "                reserved_tokens = {}\n",
    "            \n",
    "            reserved_tokens[\"unk\"] = reserved_tokens.get(\"unk\", \"<unk>\")\n",
    "            reserved_tokens[\"pad\"] = reserved_tokens.get(\"pad\", \"<pad>\")\n",
    "            reserved_tokens[\"start\"] = reserved_tokens.get(\"start\", \"<s>\")\n",
    "            reserved_tokens[\"end\"] = reserved_tokens.get(\"end\", \"</s>\")\n",
    "\n",
    "            self.start_token, self.token2id[reserved_tokens['start']] = reserved_tokens[\"start\"], 1\n",
    "            self.end_token, self.token2id[reserved_tokens['end']] = reserved_tokens[\"end\"], 2\n",
    "            self.unk, self.token2id[reserved_tokens['unk']] = reserved_tokens[\"unk\"], 3\n",
    "            self.pad, self.token2id[reserved_tokens['pad']] = reserved_tokens[\"pad\"], 0\n",
    "\n",
    "        if not tokens:\n",
    "            tokens = []\n",
    "\n",
    "        self.id2word = {}\n",
    "        uniq_tokens = list(self.token2id.keys())\n",
    "        token_freqs = count_corpus(tokens)\n",
    "        uniq_tokens += [token for token, freq in token_freqs\n",
    "                        if freq >= min_freq and token not in uniq_tokens]\n",
    "\n",
    "        for token in uniq_tokens:\n",
    "            self.token2id[token] = self.token2id.get(token, len(self.token2id))\n",
    "            self.id2word[self.token2id[token]] = token\n",
    "\n",
    "        # For handling chars\n",
    "\n",
    "        self.char_list = list(\n",
    "            \"\"\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]\"\"\"\n",
    "        )\n",
    "        self.char2id = {}\n",
    "        self.char2id[self.pad] = 0\n",
    "        self.start_char, self.char2id[\"{\"] = \"{\", 1\n",
    "        self.end_char, self.char2id[\"}\"] = \"}\", 2\n",
    "        self.char2id[self.unk] = 3\n",
    "\n",
    "        for c in self.char_list:\n",
    "            self.char2id[c] = self.char2id.get(c, len(self.char2id))\n",
    "        \n",
    "        self.id2char = {v:k for k,v in self.char2id.items()}\n",
    "        \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.token2id)\n",
    "\n",
    "    def __getitem__(self, tokens: Union[List[str], Tuple[str], str]) -> Union[List[int], int]:\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token2id.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def __contains__(self, token) -> bool:\n",
    "        return token in self.token2id\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        raise ValueError(\"Vocabulary store is read only\")\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Vocab Store: Tokens [size={len(self)}], Characters [size={len(self.char2id)}]\"\n",
    "\n",
    "    def to_tokens(self, indices: Union[List[int], Tuple[int], int]) -> Union[List[int], int]:\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.id2word.get(indices, None)\n",
    "        return [self.to_tokens(index) for index in indices]\n",
    "\n",
    "    def len(self, tokens: bool = True) -> int:\n",
    "        return len(self.token2id) if tokens else len(self.char2id)\n",
    "\n",
    "    def sent2id(self, sents: List[List[str]]) -> List[List[int]]:\n",
    "        return [self[sent] for sent in sents]\n",
    "\n",
    "    def to_charid(self, char: Union[List[str], str]) -> int:\n",
    "        if not isinstance(char, (list, tuple)):\n",
    "            return self.char2id.get(char, self.unk)\n",
    "        return [self.to_charid(c) for c in char]\n",
    "\n",
    "    def word2char(self, tokens: [Union[List[str], str]]) -> Union[List[List[int]], List[int]]:\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return [self.char2id.get(char, self.unk) for char in self.start_char + tokens + self.end_char]\n",
    "        return [self.word2char(token) for token in tokens]\n",
    "\n",
    "    def to_char(self, indices: Union[int, List[int]]) -> Union[str, List[str]]:\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.id2char.get(indices, None)\n",
    "        return [self.to_char(index) for index in indices]\n",
    "\n",
    "    def sent2charid(self, sents: List[List[str]]) -> List[List[List[int]]]:\n",
    "        return [self.word2char(sent) for sent in sents]\n",
    "\n",
    "    def to_tensor(self, sents: List[List[str]], tokens: bool, device: torch.device) -> torch.Tensor:\n",
    "        ids = self.sent2id(sents) if tokens else self.sent2charid(sents)\n",
    "        pad_ids = pad_sents(ids, self[self.pad]) if tokens else pad_sents_char(ids, self.to_charid(self.pad))\n",
    "        tensor_sents = torch.tensor(pad_ids, dtype=torch.long, device=device)\n",
    "        return torch.t(tensor_sents) if tokens else tensor_sents.permute([1, 0, 2])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab Store: Tokens [size=17], Characters [size=96]\n[('<s>', 1), ('</s>', 2), ('<unk>', 3), ('<pad>', 0), ('do', 4), ('we', 5), ('Computer:', 6), ('Human:', 7), ('When', 8), ('want', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab_store = VocabStore(sentences_words)\n",
    "print(vocab_store)\n",
    "print(list(vocab_store.token2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nFalse\n"
     ]
    }
   ],
   "source": [
    "print(\"we\" in vocab_store)\n",
    "print(\"NOTWE\" in vocab_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[7, 11, 4, 5, 15]\n"
     ]
    }
   ],
   "source": [
    "print(vocab_store[sentences_words[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Human:', 'What', 'do', 'we', 'want?']\n"
     ]
    }
   ],
   "source": [
    "print(vocab_store.to_tokens([7, 11, 4, 5, 15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[7, 11, 4, 5, 15], [6, 10, 13, 14], [7, 8, 4, 5, 9, 12], [6, 8, 4, 5, 9, 16]]"
      ]
     },
     "metadata": {},
     "execution_count": 172
    }
   ],
   "source": [
    "vocab_store.sent2id(sentences_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[7, 11, 4, 5, 15, 0],\n",
       " [6, 10, 13, 14, 0, 0],\n",
       " [7, 8, 4, 5, 9, 12],\n",
       " [6, 8, 4, 5, 9, 16]]"
      ]
     },
     "metadata": {},
     "execution_count": 173
    }
   ],
   "source": [
    "pad_sents(vocab_store.sent2id(sentences_words), vocab_store[vocab_store.pad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Human:', 'What', 'do', 'we', 'want?']"
      ]
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "vocab_store.to_tokens([7, 11, 4, 5, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[1, 11, 50, 42, 30, 43, 71, 2],\n",
       " [1, 26, 37, 30, 49, 2],\n",
       " [1, 33, 44, 2],\n",
       " [1, 52, 34, 2],\n",
       " [1, 52, 30, 43, 49, 70, 2]]"
      ]
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "vocab_store.word2char(sentences_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['{', 'G', 't', 'l', 'Z', 'm', '?', '}']"
      ]
     },
     "metadata": {},
     "execution_count": 176
    }
   ],
   "source": [
    "vocab_store.to_char([1, 10, 49, 41, 29, 42, 70, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[[1, 11, 50, 42, 30, 43, 71, 2],\n",
       "  [1, 26, 37, 30, 49, 2],\n",
       "  [1, 33, 44, 2],\n",
       "  [1, 52, 34, 2],\n",
       "  [1, 52, 30, 43, 49, 70, 2]],\n",
       " [[1, 6, 44, 42, 45, 50, 49, 34, 47, 71, 2],\n",
       "  [1, 17, 30, 49, 50, 47, 30, 41, 2],\n",
       "  [1, 41, 30, 43, 36, 50, 30, 36, 34, 2],\n",
       "  [1, 45, 47, 44, 32, 34, 48, 48, 38, 43, 36, 69, 2]],\n",
       " [[1, 11, 50, 42, 30, 43, 71, 2],\n",
       "  [1, 26, 37, 34, 43, 2],\n",
       "  [1, 33, 44, 2],\n",
       "  [1, 52, 34, 2],\n",
       "  [1, 52, 30, 43, 49, 2],\n",
       "  [1, 38, 49, 70, 2]],\n",
       " [[1, 6, 44, 42, 45, 50, 49, 34, 47, 71, 2],\n",
       "  [1, 26, 37, 34, 43, 2],\n",
       "  [1, 33, 44, 2],\n",
       "  [1, 52, 34, 2],\n",
       "  [1, 52, 30, 43, 49, 2],\n",
       "  [1, 52, 37, 30, 49, 70, 2]]]"
      ]
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "vocab_store.sent2charid(sentences_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[[1, 11, 50, 42, 30, 43, 71, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 26, 37, 30, 49, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 33, 44, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 34, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 30, 43, 49, 70, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [[1, 6, 44, 42, 45, 50, 49, 34, 47, 71, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 17, 30, 49, 50, 47, 30, 41, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 41, 30, 43, 36, 50, 30, 36, 34, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 45, 47, 44, 32, 34, 48, 48, 38, 43, 36, 69, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [[1, 11, 50, 42, 30, 43, 71, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 26, 37, 34, 43, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 33, 44, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 34, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 30, 43, 49, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 38, 49, 70, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [[1, 6, 44, 42, 45, 50, 49, 34, 47, 71, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 26, 37, 34, 43, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 33, 44, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 34, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 30, 43, 49, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 52, 37, 30, 49, 70, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]"
      ]
     },
     "metadata": {},
     "execution_count": 178
    }
   ],
   "source": [
    "pad_sents_char(vocab_store.sent2charid(sentences_words), vocab_store.to_charid(vocab_store.pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([6, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "source": [
    "vocab_store.to_tensor(sentences_words, tokens=True, device=\"cpu\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([6, 4, 21])"
      ]
     },
     "metadata": {},
     "execution_count": 184
    }
   ],
   "source": [
    "vocab_store.to_tensor(sentences_words, tokens=False, device=\"cpu\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}