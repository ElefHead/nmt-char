{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from nmt.datasets import Vocab, batch_iter\n",
    "from nmt.networks import CharEmbedding, Encoder\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "source": [
    "### Setting up everything till before we need to use attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample data\n",
    "sentences_words_src = [\n",
    "    ['Human:', 'What', 'do', 'we', 'want?'],\n",
    "    ['Computer:', 'Natural', 'language', 'processing!'],\n",
    "    ['Human:', 'When', 'do', 'we', 'want', 'it?'],\n",
    "    ['Computer:', 'When', 'do', 'we', 'want', 'what?']\n",
    "]\n",
    "\n",
    "sentences_words_tgt = [\n",
    "    ['<s>', 'Human:', 'What', 'do', 'we', 'want?', '</s>'],\n",
    "    ['<s>', 'Computer:', 'Natural', 'language', 'processing!', '</s>'],\n",
    "    ['<s>', 'Human:', 'When', 'do', 'we', 'want', 'it?', '</s>'],\n",
    "    ['<s>', 'Computer:', 'When', 'do', 'we', 'want', 'what?', '</s>']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing source vocab\nVocab Store: Tokens [size=17],                 Characters [size=97]\nInitializing target vocab\nVocab Store: Tokens [size=17],                 Characters [size=97]\n"
     ]
    }
   ],
   "source": [
    "## Setting up vocab\n",
    "vocab = Vocab.build(sentences_words_src, sentences_words_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['Human:', 'When', 'do', 'we', 'want', 'it?'], ['Computer:', 'When', 'do', 'we', 'want', 'what?'], ['Human:', 'What', 'do', 'we', 'want?'], ['Computer:', 'Natural', 'language', 'processing!']]\n"
     ]
    }
   ],
   "source": [
    "## Generating a batch\n",
    "data = list(zip(sentences_words_src, sentences_words_tgt))\n",
    "data_generator = batch_iter(\n",
    "    data=data,\n",
    "    batch_size=4,\n",
    "    shuffle=True\n",
    ")\n",
    "batch_src, batch_tgt = next(data_generator)\n",
    "print(batch_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[6, 6, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "## Getting source lengths for encoder\n",
    "source_length = [len(sent) for sent in batch_src]\n",
    "print(source_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "src char tensor size = torch.Size([6, 4, 21]); tgt char tensor size = torch.Size([8, 4, 21])\n"
     ]
    }
   ],
   "source": [
    "## Preparing input and output tensors\n",
    "char_tensors_src = vocab.src.to_tensor(batch_src, tokens=False)\n",
    "char_tensors_tgt = vocab.tgt.to_tensor(batch_tgt, tokens=False)\n",
    "print(f\"src char tensor size = {char_tensors_src.size()}; tgt char tensor size = {char_tensors_tgt.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining Encoder\n",
    "encoder = Encoder(\n",
    "    num_embeddings=vocab.src.length(tokens=False),\n",
    "    embedding_dim=300,\n",
    "    char_padding_idx=vocab.src.pad_char_idx,\n",
    "    hidden_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([4, 6, 2048]), torch.Size([4, 1024]), torch.Size([4, 1024]))"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "## Getting encoder output\n",
    "char_enc_hidden, (char_hidden, char_cell) = encoder(char_tensors_src, source_length)\n",
    "char_enc_hidden.shape, char_hidden.shape, char_cell.shape"
   ]
  },
  {
   "source": [
    "Note here: We have encoder output equivalent of 6 timesteps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Component of decoder - Target embedding layer\n",
    "target_embedding = CharEmbedding(\n",
    "    num_embeddings=vocab.tgt.length(tokens=False),\n",
    "    char_embedding_dim=50,\n",
    "    embedding_dim=300,\n",
    "    char_padding_idx=vocab.tgt.pad_char_idx    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Component of decoder - decoder LSTM Cell\n",
    "sample_decoder_cell = nn.LSTMCell(\n",
    "    input_size=300 + 1024,\n",
    "    hidden_size=1024, bias=True\n",
    ")"
   ]
  },
  {
   "source": [
    "When we decode, we do it one time-step at a time. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0 = torch.split(char_tensors_tgt, 1, dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Target tensor shape: torch.Size([1, 4, 21])\nTarget embedded shape: torch.Size([1, 4, 300])\n"
     ]
    }
   ],
   "source": [
    "print(\"Target tensor shape:\", y_0.shape)\n",
    "embedded_y_0 = target_embedding(y_0)\n",
    "print(\"Target embedded shape:\", embedded_y_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_prev = torch.zeros(4, 1024, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4, 1324])\n"
     ]
    }
   ],
   "source": [
    "ybar_t = torch.cat([embedded_y_0.squeeze(dim=0), o_prev], dim=1)\n",
    "print(ybar_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_hidden, dec_cell = sample_decoder_cell(ybar_t, (char_hidden, char_cell)) # initial dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([4, 1024]), torch.Size([4, 1024]))"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "dec_hidden.shape, dec_cell.shape"
   ]
  },
  {
   "source": [
    "We have encoder output corresponding to _all_ timesteps and decoder output corresponding to _one_ timestep."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# General Attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### What is Attention? \n",
    "\n",
    "There are a million explanations of attention and my favorite interpretation \n",
    "given that attention essentially talks of information retention and retrieval is that \n",
    "attention is a mechanism for mapping a query to a value corresponding to all relevant \n",
    "values in a set.  \n",
    "  \n",
    "The following image shows the computation of attention and context vectors using dot product attention.\n",
    "\n",
    "<img src=\"images/attention.png\" />\n",
    "\n",
    "  \n",
    "### What is general attention? \n",
    "\n",
    "Note that our decoder hidden values (query) are of size hidden_size = 1024 but encoder outputs are 2*hidden_size = 2048. There are many cases in which this could occur. Therefore, we use a linear layer to project the encoder outputs to match the decoder hidden values. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([4, 6, 2048]), torch.Size([4, 1024]))"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "char_enc_hidden.shape, dec_hidden.shape"
   ]
  },
  {
   "source": [
    "Here, we will say the encoder output *s = char_enc_hidden = the set*  \n",
    "The decoder hidden *h_i = query*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_projection = nn.Linear(in_features=2048, out_features=1024, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_projection = attention_projection(char_enc_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "enc_projection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_hidden_unsqueezed = dec_hidden.unsqueeze(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 1024, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "dec_hidden_unsqueezed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
    "score = enc_projection.bmm(dec_hidden_unsqueezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = score.squeeze(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = F.softmax(score, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = attention_weights.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
    "> If input is a (b×n×m) tensor, mat2 is a (b×m×p) tensor, out will be a (b×n×p) tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector = attention_weights.bmm(char_enc_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 2048])"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "context_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector = context_vector.squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 2048])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "context_vector.shape"
   ]
  },
  {
   "source": [
    "## Putting it together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                enc_hidden: torch.Tensor,\n",
    "                dec_hidden_t: torch.Tensor,\n",
    "                enc_masks: torch.Tensor = None) -> torch.Tensor:\n",
    "\n",
    "        enc_projection = self.linear(enc_hidden)\n",
    "        dec_hidden_unsqueezed_t = dec_hidden_t.unsqueeze(dim=2)\n",
    "        score = enc_projection.bmm(dec_hidden_unsqueezed_t)\n",
    "        score = score.squeeze(dim=2)\n",
    "\n",
    "        if enc_masks is not None:\n",
    "            score.data.masked_fill_(\n",
    "                enc_masks.byte(),\n",
    "                -float('inf')\n",
    "            )\n",
    "\n",
    "        attention_weights = softmax(score, dim=1)\n",
    "        attention_weights = attention_weights.unsqueeze(dim=1)\n",
    "\n",
    "        context_vector = attention_weights.bmm(enc_hidden)\n",
    "        context_vector = context_vector.squeeze(dim=1)\n",
    "\n",
    "        return attention_weights, context_vector"
   ]
  },
  {
   "source": [
    "### Wait what is that mask thing? \n",
    "\n",
    "It's the encoder mask.  \n",
    "\n",
    "Because of batch thing, some sentences will be shorter than the longest sentence with remaining words having pad indices. We don't want attention to focus on those parts and therefore we set the parts corresponding to pad_indices to -inf because softmax(-inf) = 0. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}