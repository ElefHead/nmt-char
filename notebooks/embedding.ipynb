{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from nmt.datasets import Vocab, VocabStore"
   ]
  },
  {
   "source": [
    "#### From Character-Aware Neural Language Models : Kim et al (https://arxiv.org/pdf/1508.06615.pdf)\n",
    "\n",
    "<img src=\"images/cnn-char-model.png\" />\n",
    "\n",
    "> Whereas a conventional NLM takes word embeddings as inputs, our model instead takes the output from a single-layer character-level convolutional neural network with max-over-time pooling.\n",
    "\n",
    "Here we will implement the embedding upto the highway network. Everything uptil the LSTM.  \n",
    "* Character Embedding Layer\n",
    "* CNN Embedding Layer with Maxpool\n",
    "* Highway Network\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup something to work with\n",
    "\n",
    "sentences = [\n",
    "    \"Human: What do we want?\",\n",
    "    \"Computer: Natural language processing!\",\n",
    "    \"Human: When do we want it?\",\n",
    "    \"Computer: When do we want what?\"\n",
    "]\n",
    "\n",
    "sentences_words = [\n",
    "    ['Human:', 'What', 'do', 'we', 'want?'],\n",
    "    ['Computer:', 'Natural', 'language', 'processing!'],\n",
    "    ['Human:', 'When', 'do', 'we', 'want', 'it?'],\n",
    "    ['Computer:', 'When', 'do', 'we', 'want', 'what?']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing source vocab\nVocab Store: Tokens [size=17],                 Characters [size=97]\nInitializing target vocab\nVocab Store: Tokens [size=17],                 Characters [size=97]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab.build(sentences, sentences_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tensors = vocab.src.to_tensor(sentences_words, tokens=False)"
   ]
  },
  {
   "source": [
    "I input 4 sentences. Max words per sentence is 21. Max sentence length is 6.  \n",
    "Therefore, tensor shape is (6, 4, 21)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([6, 4, 21])\n"
     ]
    }
   ],
   "source": [
    "print(char_tensors.size())"
   ]
  },
  {
   "source": [
    "<img src=\"images/cnn-char-layer.png\" />\n",
    "\n",
    "```python\n",
    "C = vocab.src.length(tokens=False) # 97 in this case\n",
    "d = 50 # usually good enough for character embeddings\n",
    "l = 21 # we set max word length in vocab\n",
    "```\n",
    "\n",
    "According to paper the character-level representation of word k will of dimension d x l = [50 x 21]. \n",
    "  \n",
    "Note Q is just the char embedding layer. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Char Embedding (Q)\n",
    "embed = nn.Embedding(num_embeddings=vocab.src.length(tokens=False), embedding_dim=50, padding_idx=vocab.src.pad_char_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Embedding\n",
    "class CharCNNEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 5) -> None:\n",
    "        super(CharCNNEmbedding, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=in_channels, out_channels=out_channels,\n",
    "                              kernel_size=kernel_size)\n",
    "        self.maxpool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        out = F.relu(self.conv(x))\n",
    "        out = self.maxpool(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnembed = CharCNNEmbedding(in_channels=50, out_channels=1024, kernel_size=5)"
   ]
  },
  {
   "source": [
    "<img src=\"images/char-embed-highway.png\" />\n",
    "\n",
    "It's essentially a residual layer but with a gate. Hence the highway."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Highway Layer\n",
    "class Highway(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int) -> None:\n",
    "        super(Highway, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=in_features, out_features=out_features)\n",
    "        self.gate = nn.Linear(in_features=in_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = F.relu(self.linear(x))\n",
    "        t = torch.sigmoid(self.gate(x))\n",
    "\n",
    "        return t * z + (1 - t) * x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "highway = Highway(in_features=1024, out_features=1024)"
   ]
  },
  {
   "source": [
    "Alright lets check this out."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embed = embed(char_tensors)"
   ]
  },
  {
   "source": [
    "This would be of shape (sentence_length, batch_size, num_words, num_embeddings)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([6, 4, 21, 50])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "char_embed.size()"
   ]
  },
  {
   "source": [
    "Recall\n",
    "> According to paper the character-level representation of word k will of dimension d x l = [50 x 21]. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([24, 50, 21])\n"
     ]
    }
   ],
   "source": [
    "char_embed = char_embed.view(-1, 21, 50).permute(0, 2, 1)\n",
    "print(char_embed.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_embed_tensor = cnnembed(char_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([24, 1024, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "cnn_embed_tensor.size()"
   ]
  },
  {
   "source": [
    "To feed to linear layer, gotta squeeze it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([24, 1024])\n"
     ]
    }
   ],
   "source": [
    "cnn_embed_tensor = cnn_embed_tensor.squeeze(dim=2)\n",
    "print(cnn_embed_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "highway_embed_tensor = highway(cnn_embed_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([24, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "highway_embed_tensor.size()"
   ]
  },
  {
   "source": [
    "Now convert it back to (sentence_length, batch_size, embeddings)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([6, 4, 1024])\n"
     ]
    }
   ],
   "source": [
    "## Note when we actually feed to lstm, we will pack padded \n",
    "## https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
    "input_to_lstm = highway_embed_tensor.view(6, 4, 1024)\n",
    "print(input_to_lstm.size())"
   ]
  },
  {
   "source": [
    "### Put it together as a CharEmbeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, char_embedding_dim: int,\n",
    "                 embedding_dim: int, char_padding_idx: int,\n",
    "                 cnn_kernel_size: int = 5,\n",
    "                 dropout_prob: float = 0.3) -> None:\n",
    "        super(CharEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "\n",
    "        self.char_embed = nn.Embedding(\n",
    "            num_embeddings=num_embeddings,\n",
    "            embedding_dim=char_embedding_dim,\n",
    "            padding_idx=char_padding_idx\n",
    "        )\n",
    "        self.cnn_embed = CharCNNEmbedding(\n",
    "            in_channels=char_embedding_dim,\n",
    "            out_channels=embedding_dim,\n",
    "            kernel_size=cnn_kernel_size\n",
    "        )\n",
    "        self.highway = Highway(\n",
    "            in_features=embedding_dim,\n",
    "            out_features=embedding_dim\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sentence_length, batch_size, word_length = x.size()\n",
    "\n",
    "        embed_t = self.char_embed(x)\n",
    "        embed_t = embed_t.view(-1, word_length,\n",
    "                               self.char_embedding_dim).permute([0, 2, 1])\n",
    "\n",
    "        cnn_embed_t = self.cnn_embed(embed_t)\n",
    "        cnn_embed_t = cnn_embed_t.squeeze(dim=2)\n",
    "\n",
    "        highway_t = self.highway(cnn_embed_t)\n",
    "\n",
    "        out = self.dropout(highway_t)\n",
    "\n",
    "        out = out.view(sentence_length, batch_size, self.embedding_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embeddings = CharEmbedding(num_embeddings=vocab.src.length(tokens=False), char_embedding_dim=50, embedding_dim=1024, char_padding_idx=vocab.src.pad_char_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_to_lstm = model_embeddings(char_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([6, 4, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "inp_to_lstm.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}