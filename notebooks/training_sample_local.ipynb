{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from nmt.models import NMT\n",
    "from nmt.datasets import read_corpus, batch_iter, Vocab\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = Path(\"..\") / \"nmt\" / \"datasets\" / \"data\"\n",
    "en_es_data_loc = data_loc / \"en_es_data\"\n",
    "train_data_src_path = en_es_data_loc / \"train_tiny.es\"\n",
    "train_data_tgt_path = en_es_data_loc / \"train_tiny.en\"\n",
    "dev_data_src_path = en_es_data_loc / \"dev_tiny.es\"\n",
    "dev_data_tgt_path = en_es_data_loc / \"dev_tiny.en\"\n",
    "vocab_path = data_loc / \"vocab_tiny_q2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = read_corpus(train_data_src_path)\n",
    "train_tgt = read_corpus(train_data_tgt_path, is_target=True)\n",
    "vocab = Vocab.load(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=2\n",
    "MAX_EPOCH=201\n",
    "SEED=42\n",
    "EMBEDDING_SIZE=256\n",
    "HIDDEN_SIZE=256\n",
    "GRAD_CLIP=5.0\n",
    "UNIFORM_INIT=0.1\n",
    "USE_CHAR_DECODER=True\n",
    "LEARNING_RATE=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NMT(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): CharEmbedding(\n",
       "      (char_embed): Embedding(97, 50, padding_idx=0)\n",
       "      (cnn_embed): CharCNNEmbedding(\n",
       "        (conv): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
       "        (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
       "      )\n",
       "      (highway): Highway(\n",
       "        (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (gate): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (encoder): LSTM(256, 256, num_layers=2, bidirectional=True)\n",
       "    (hidden_projection): Linear(in_features=512, out_features=256, bias=False)\n",
       "    (cell_projection): Linear(in_features=512, out_features=256, bias=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): CharEmbedding(\n",
       "      (char_embed): Embedding(97, 50, padding_idx=0)\n",
       "      (cnn_embed): CharCNNEmbedding(\n",
       "        (conv): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
       "        (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
       "      )\n",
       "      (highway): Highway(\n",
       "        (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (gate): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (attention): Attention(\n",
       "      (linear): Linear(in_features=512, out_features=256, bias=False)\n",
       "    )\n",
       "    (decoder): LSTMCell(512, 256)\n",
       "    (combined_projection): Linear(in_features=768, out_features=256, bias=False)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (target_layer): Linear(in_features=256, out_features=32, bias=False)\n",
       "  (char_decoder): CharDecoder(\n",
       "    (embedding): Embedding(97, 50, padding_idx=0)\n",
       "    (char_decoder): LSTM(50, 256)\n",
       "    (linear): Linear(in_features=256, out_features=97, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "model = NMT(\n",
    "    vocab=vocab,\n",
    "    embedding_dim=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    use_char_decoder=True\n",
    ")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "uniformly initialize parameters [-0.100000, +0.100000]\n"
     ]
    }
   ],
   "source": [
    "uniform_init = UNIFORM_INIT\n",
    "if np.abs(uniform_init) > 0.:\n",
    "    print('uniformly initialize parameters [-%f, +%f]' %\n",
    "            (uniform_init, uniform_init), file=sys.stderr)\n",
    "    for p in model.parameters():\n",
    "        p.data.uniform_(-uniform_init, uniform_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 000 - Cumulative loss: 72.90512084960938\n",
      "Epoch: 001 - Cumulative loss: 69.88958740234375\n",
      "Epoch: 002 - Cumulative loss: 68.48753356933594\n",
      "Epoch: 003 - Cumulative loss: 66.44123840332031\n",
      "Epoch: 004 - Cumulative loss: 63.33734130859375\n",
      "Epoch: 005 - Cumulative loss: 60.675079345703125\n",
      "Epoch: 006 - Cumulative loss: 57.88020706176758\n",
      "Epoch: 007 - Cumulative loss: 54.3707389831543\n",
      "Epoch: 008 - Cumulative loss: 51.51618194580078\n",
      "Epoch: 009 - Cumulative loss: 49.38239669799805\n",
      "Epoch: 010 - Cumulative loss: 48.00582504272461\n",
      "Epoch: 011 - Cumulative loss: 47.20466995239258\n",
      "Epoch: 012 - Cumulative loss: 46.401790618896484\n",
      "Epoch: 013 - Cumulative loss: 46.17249298095703\n",
      "Epoch: 014 - Cumulative loss: 45.65177917480469\n",
      "Epoch: 015 - Cumulative loss: 45.349369049072266\n",
      "Epoch: 016 - Cumulative loss: 44.933319091796875\n",
      "Epoch: 017 - Cumulative loss: 44.613555908203125\n",
      "Epoch: 018 - Cumulative loss: 44.37868118286133\n",
      "Epoch: 019 - Cumulative loss: 44.22948455810547\n",
      "Epoch: 020 - Cumulative loss: 44.304443359375\n",
      "Epoch: 021 - Cumulative loss: 43.887271881103516\n",
      "Epoch: 022 - Cumulative loss: 43.76502227783203\n",
      "Epoch: 023 - Cumulative loss: 43.58832550048828\n",
      "Epoch: 024 - Cumulative loss: 43.533260345458984\n",
      "Epoch: 025 - Cumulative loss: 43.402462005615234\n",
      "Epoch: 026 - Cumulative loss: 42.97931671142578\n",
      "Epoch: 027 - Cumulative loss: 43.01777267456055\n",
      "Epoch: 028 - Cumulative loss: 42.80704879760742\n",
      "Epoch: 029 - Cumulative loss: 42.736412048339844\n",
      "Epoch: 030 - Cumulative loss: 42.63886642456055\n",
      "Epoch: 031 - Cumulative loss: 42.60348129272461\n",
      "Epoch: 032 - Cumulative loss: 42.53913497924805\n",
      "Epoch: 033 - Cumulative loss: 42.434669494628906\n",
      "Epoch: 034 - Cumulative loss: 42.12822723388672\n",
      "Epoch: 035 - Cumulative loss: 42.09871292114258\n",
      "Epoch: 036 - Cumulative loss: 41.77119827270508\n",
      "Epoch: 037 - Cumulative loss: 42.001495361328125\n",
      "Epoch: 038 - Cumulative loss: 41.43141555786133\n",
      "Epoch: 039 - Cumulative loss: 41.5140380859375\n",
      "Epoch: 040 - Cumulative loss: 41.07096862792969\n",
      "Epoch: 041 - Cumulative loss: 41.32276153564453\n",
      "Epoch: 042 - Cumulative loss: 41.09172821044922\n",
      "Epoch: 043 - Cumulative loss: 41.0850830078125\n",
      "Epoch: 044 - Cumulative loss: 40.863712310791016\n",
      "Epoch: 045 - Cumulative loss: 40.72026443481445\n",
      "Epoch: 046 - Cumulative loss: 40.508750915527344\n",
      "Epoch: 047 - Cumulative loss: 40.359886169433594\n",
      "Epoch: 048 - Cumulative loss: 40.446773529052734\n",
      "Epoch: 049 - Cumulative loss: 40.035850524902344\n",
      "Epoch: 050 - Cumulative loss: 39.76587677001953\n",
      "Epoch: 051 - Cumulative loss: 39.5994873046875\n",
      "Epoch: 052 - Cumulative loss: 39.30113983154297\n",
      "Epoch: 053 - Cumulative loss: 39.23981475830078\n",
      "Epoch: 054 - Cumulative loss: 39.052574157714844\n",
      "Epoch: 055 - Cumulative loss: 38.875308990478516\n",
      "Epoch: 056 - Cumulative loss: 38.05309295654297\n",
      "Epoch: 057 - Cumulative loss: 37.925331115722656\n",
      "Epoch: 058 - Cumulative loss: 37.76670837402344\n",
      "Epoch: 059 - Cumulative loss: 37.32026290893555\n",
      "Epoch: 060 - Cumulative loss: 37.32517623901367\n",
      "Epoch: 061 - Cumulative loss: 37.11461639404297\n",
      "Epoch: 062 - Cumulative loss: 37.17258071899414\n",
      "Epoch: 063 - Cumulative loss: 36.45692443847656\n",
      "Epoch: 064 - Cumulative loss: 35.862422943115234\n",
      "Epoch: 065 - Cumulative loss: 35.70417022705078\n",
      "Epoch: 066 - Cumulative loss: 35.55574417114258\n",
      "Epoch: 067 - Cumulative loss: 35.5794677734375\n",
      "Epoch: 068 - Cumulative loss: 35.09804153442383\n",
      "Epoch: 069 - Cumulative loss: 34.42461395263672\n",
      "Epoch: 070 - Cumulative loss: 34.671775817871094\n",
      "Epoch: 071 - Cumulative loss: 34.05173873901367\n",
      "Epoch: 072 - Cumulative loss: 33.58864974975586\n",
      "Epoch: 073 - Cumulative loss: 33.27397918701172\n",
      "Epoch: 074 - Cumulative loss: 32.871707916259766\n",
      "Epoch: 075 - Cumulative loss: 32.95669937133789\n",
      "Epoch: 076 - Cumulative loss: 32.28187942504883\n",
      "Epoch: 077 - Cumulative loss: 31.99941635131836\n",
      "Epoch: 078 - Cumulative loss: 31.508169174194336\n",
      "Epoch: 079 - Cumulative loss: 31.40194320678711\n",
      "Epoch: 080 - Cumulative loss: 31.024967193603516\n",
      "Epoch: 081 - Cumulative loss: 30.729145050048828\n",
      "Epoch: 082 - Cumulative loss: 30.406051635742188\n",
      "Epoch: 083 - Cumulative loss: 30.11916160583496\n",
      "Epoch: 084 - Cumulative loss: 29.984020233154297\n",
      "Epoch: 085 - Cumulative loss: 29.399551391601562\n",
      "Epoch: 086 - Cumulative loss: 29.1934757232666\n",
      "Epoch: 087 - Cumulative loss: 28.87334632873535\n",
      "Epoch: 088 - Cumulative loss: 28.899677276611328\n",
      "Epoch: 089 - Cumulative loss: 28.400772094726562\n",
      "Epoch: 090 - Cumulative loss: 28.1895751953125\n",
      "Epoch: 091 - Cumulative loss: 27.7598934173584\n",
      "Epoch: 092 - Cumulative loss: 27.653051376342773\n",
      "Epoch: 093 - Cumulative loss: 27.36850357055664\n",
      "Epoch: 094 - Cumulative loss: 26.707632064819336\n",
      "Epoch: 095 - Cumulative loss: 26.695514678955078\n",
      "Epoch: 096 - Cumulative loss: 26.28739356994629\n",
      "Epoch: 097 - Cumulative loss: 25.748699188232422\n",
      "Epoch: 098 - Cumulative loss: 25.97231101989746\n",
      "Epoch: 099 - Cumulative loss: 24.926130294799805\n",
      "Epoch: 100 - Cumulative loss: 24.79604148864746\n",
      "Epoch: 101 - Cumulative loss: 24.852264404296875\n",
      "Epoch: 102 - Cumulative loss: 24.317819595336914\n",
      "Epoch: 103 - Cumulative loss: 24.053682327270508\n",
      "Epoch: 104 - Cumulative loss: 23.867168426513672\n",
      "Epoch: 105 - Cumulative loss: 23.153587341308594\n",
      "Epoch: 106 - Cumulative loss: 23.08991050720215\n",
      "Epoch: 107 - Cumulative loss: 22.873291015625\n",
      "Epoch: 108 - Cumulative loss: 22.183988571166992\n",
      "Epoch: 109 - Cumulative loss: 21.963411331176758\n",
      "Epoch: 110 - Cumulative loss: 21.75689697265625\n",
      "Epoch: 111 - Cumulative loss: 21.34843635559082\n",
      "Epoch: 112 - Cumulative loss: 21.1742000579834\n",
      "Epoch: 113 - Cumulative loss: 20.79738998413086\n",
      "Epoch: 114 - Cumulative loss: 20.648366928100586\n",
      "Epoch: 115 - Cumulative loss: 20.218042373657227\n",
      "Epoch: 116 - Cumulative loss: 20.23306655883789\n",
      "Epoch: 117 - Cumulative loss: 20.055253982543945\n",
      "Epoch: 118 - Cumulative loss: 19.257320404052734\n",
      "Epoch: 119 - Cumulative loss: 19.121374130249023\n",
      "Epoch: 120 - Cumulative loss: 18.606170654296875\n",
      "Epoch: 121 - Cumulative loss: 18.610382080078125\n",
      "Epoch: 122 - Cumulative loss: 17.850872039794922\n",
      "Epoch: 123 - Cumulative loss: 18.12105941772461\n",
      "Epoch: 124 - Cumulative loss: 17.68355941772461\n",
      "Epoch: 125 - Cumulative loss: 17.02553939819336\n",
      "Epoch: 126 - Cumulative loss: 17.45479393005371\n",
      "Epoch: 127 - Cumulative loss: 16.96010971069336\n",
      "Epoch: 128 - Cumulative loss: 16.250837326049805\n",
      "Epoch: 129 - Cumulative loss: 16.401935577392578\n",
      "Epoch: 130 - Cumulative loss: 16.035884857177734\n",
      "Epoch: 131 - Cumulative loss: 15.717428207397461\n",
      "Epoch: 132 - Cumulative loss: 15.032740592956543\n",
      "Epoch: 133 - Cumulative loss: 15.47636890411377\n",
      "Epoch: 134 - Cumulative loss: 14.665884017944336\n",
      "Epoch: 135 - Cumulative loss: 14.46086311340332\n",
      "Epoch: 136 - Cumulative loss: 14.48875617980957\n",
      "Epoch: 137 - Cumulative loss: 14.047723770141602\n",
      "Epoch: 138 - Cumulative loss: 14.354799270629883\n",
      "Epoch: 139 - Cumulative loss: 13.705602645874023\n",
      "Epoch: 140 - Cumulative loss: 13.499765396118164\n",
      "Epoch: 141 - Cumulative loss: 13.13695240020752\n",
      "Epoch: 142 - Cumulative loss: 12.827984809875488\n",
      "Epoch: 143 - Cumulative loss: 12.402076721191406\n",
      "Epoch: 144 - Cumulative loss: 12.321266174316406\n",
      "Epoch: 145 - Cumulative loss: 12.283232688903809\n",
      "Epoch: 146 - Cumulative loss: 12.121402740478516\n",
      "Epoch: 147 - Cumulative loss: 11.668147087097168\n",
      "Epoch: 148 - Cumulative loss: 11.76070785522461\n",
      "Epoch: 149 - Cumulative loss: 11.252202987670898\n",
      "Epoch: 150 - Cumulative loss: 11.244449615478516\n",
      "Epoch: 151 - Cumulative loss: 10.542097091674805\n",
      "Epoch: 152 - Cumulative loss: 10.68138313293457\n",
      "Epoch: 153 - Cumulative loss: 10.400227546691895\n",
      "Epoch: 154 - Cumulative loss: 10.234880447387695\n",
      "Epoch: 155 - Cumulative loss: 10.278672218322754\n",
      "Epoch: 156 - Cumulative loss: 10.71182632446289\n",
      "Epoch: 157 - Cumulative loss: 10.095348358154297\n",
      "Epoch: 158 - Cumulative loss: 9.230461120605469\n",
      "Epoch: 159 - Cumulative loss: 9.148420333862305\n",
      "Epoch: 160 - Cumulative loss: 9.51130199432373\n",
      "Epoch: 161 - Cumulative loss: 8.936052322387695\n",
      "Epoch: 162 - Cumulative loss: 8.828569412231445\n",
      "Epoch: 163 - Cumulative loss: 8.642749786376953\n",
      "Epoch: 164 - Cumulative loss: 8.61906623840332\n",
      "Epoch: 165 - Cumulative loss: 8.253581047058105\n",
      "Epoch: 166 - Cumulative loss: 7.9569993019104\n",
      "Epoch: 167 - Cumulative loss: 7.976585388183594\n",
      "Epoch: 168 - Cumulative loss: 7.475293159484863\n",
      "Epoch: 169 - Cumulative loss: 7.531805515289307\n",
      "Epoch: 170 - Cumulative loss: 7.71407413482666\n",
      "Epoch: 171 - Cumulative loss: 7.254733085632324\n",
      "Epoch: 172 - Cumulative loss: 7.0277252197265625\n",
      "Epoch: 173 - Cumulative loss: 7.033505916595459\n",
      "Epoch: 174 - Cumulative loss: 6.691653251647949\n",
      "Epoch: 175 - Cumulative loss: 6.598996639251709\n",
      "Epoch: 176 - Cumulative loss: 6.302992820739746\n",
      "Epoch: 177 - Cumulative loss: 6.390624523162842\n",
      "Epoch: 178 - Cumulative loss: 6.169699192047119\n",
      "Epoch: 179 - Cumulative loss: 5.785813808441162\n",
      "Epoch: 180 - Cumulative loss: 5.932574272155762\n",
      "Epoch: 181 - Cumulative loss: 5.82115364074707\n",
      "Epoch: 182 - Cumulative loss: 5.445622444152832\n",
      "Epoch: 183 - Cumulative loss: 5.476039409637451\n",
      "Epoch: 184 - Cumulative loss: 5.2011847496032715\n",
      "Epoch: 185 - Cumulative loss: 5.289048194885254\n",
      "Epoch: 186 - Cumulative loss: 5.138621807098389\n",
      "Epoch: 187 - Cumulative loss: 4.773639678955078\n",
      "Epoch: 188 - Cumulative loss: 4.99519681930542\n",
      "Epoch: 189 - Cumulative loss: 4.683986186981201\n",
      "Epoch: 190 - Cumulative loss: 4.5595502853393555\n",
      "Epoch: 191 - Cumulative loss: 4.284634590148926\n",
      "Epoch: 192 - Cumulative loss: 4.418156623840332\n",
      "Epoch: 193 - Cumulative loss: 4.152507305145264\n",
      "Epoch: 194 - Cumulative loss: 4.100065231323242\n",
      "Epoch: 195 - Cumulative loss: 4.0534491539001465\n",
      "Epoch: 196 - Cumulative loss: 3.9165282249450684\n",
      "Epoch: 197 - Cumulative loss: 3.802051544189453\n",
      "Epoch: 198 - Cumulative loss: 3.4779446125030518\n",
      "Epoch: 199 - Cumulative loss: 3.618354082107544\n",
      "Epoch: 200 - Cumulative loss: 3.9128971099853516\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(MAX_EPOCH):\n",
    "    cum_loss = 0\n",
    "    for i, (src_sents, tgt_sents) in enumerate(batch_iter((train_src, train_tgt), batch_size=BATCH_SIZE, shuffle=True)):\n",
    "        optimizer.zero_grad()\n",
    "        batch_size = len(src_sents)\n",
    "\n",
    "        batch_loss = -model(src_sents, tgt_sents).sum()\n",
    "        batch_loss /= batch_size\n",
    "        cum_loss += batch_loss\n",
    "        batch_loss.backward()\n",
    "\n",
    "         # clip gradient\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "    cum_loss /= len(train_src)\n",
    "    print(f\"Epoch: {str(epoch).zfill(3)} - Cumulative loss: {cum_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}